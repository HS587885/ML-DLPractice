{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7NJgIKz2KPHkn4t2LeXYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/ML-DLPratice/blob/main/%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 역전파 학습법을 이용한 심층 신경망 학습"
      ],
      "metadata": {
        "id": "tLj02nSGF7hw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g-aHoQvtF1qv"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility function"
      ],
      "metadata": {
        "id": "ugBV1VePGBK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _t(x):\n",
        "  return np.transpose(x)\n",
        "\n",
        "def _m(A, B):\n",
        "  return np.matmul(A, B)"
      ],
      "metadata": {
        "id": "7ezV3vhWGAkZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sigmoid"
      ],
      "metadata": {
        "id": "rveKlDYfGIvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.last_o = 1 # 마지막 출력 저장. 1로 해주는 이유는 곱해도 값이 변하지 않음\n",
        "  \n",
        "  def __call__(self,x):\n",
        "    self.last_o = 1 / (1.0 + np.exp(-x))\n",
        "    return self.last_o\n",
        "\n",
        "  def grad(self): # sigmoid(x)(1-sigmoid(x))\n",
        "    return self.last_o * (1 - self.last_o)"
      ],
      "metadata": {
        "id": "MvgmEEz0GIOF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MSE 구현"
      ],
      "metadata": {
        "id": "wZI5nweBGRkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanSquaredError:\n",
        "  def __init__(self):\n",
        "    # 경사 저장\n",
        "    self.dh = 1\n",
        "    self.last_diff = 1\n",
        "\n",
        "  def __call__(self,h,y): # 1/2 * mean((h - y)^2)\n",
        "    self.last_diff = h - y\n",
        "    return 1/2 * np.mean(np.square(h-y))\n",
        "\n",
        "  def grad(self): # h - y\n",
        "    return self.last_diff"
      ],
      "metadata": {
        "id": "hSY_UqYjGaAf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신경망 구현"
      ],
      "metadata": {
        "id": "3k2l6dOTGYUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  def __init__(self,W,b,a_obj):\n",
        "    # 매개변수\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.a = a_obj()\n",
        "\n",
        "    # 경사 \n",
        "    self.dW = np.zeros_like(self.W)\n",
        "    self.db = np.zeros_like(self.b)\n",
        "    self.dh = np.zeros_like(_t(self.W))\n",
        "\n",
        "    self.last_x = np.zeros((self.W.shape[0]))\n",
        "    self.last_h = np.zeros((self.W.shape[1]))\n",
        "  \n",
        "  def __call__(self,x):\n",
        "    self.last_x = x\n",
        "    self.last_h = _m(_t(self.W),x) + self.b\n",
        "    return self.a(self.last_h)\n",
        "\n",
        "  def grad(self): # dy/dh = W\n",
        "    return self.W * self.a.grad()\n",
        "\n",
        "  def grad_W(self,dh):\n",
        "    grad = np.ones_like(self.W)\n",
        "    grad_a = self.a.grad()\n",
        "    for j in range(grad.shape[1]): # y = w*Tx + b, dy/dw = x\n",
        "      grad[:,j] = dh[j] + grad_a[j] * self.last_x\n",
        "    return grad\n",
        "\n",
        "  def grad_b(self,dh): # dy/dh = 1\n",
        "    return dh * self.a.grad() * 1"
      ],
      "metadata": {
        "id": "ZgnfosV_GS3I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 심층 신경망 구현"
      ],
      "metadata": {
        "id": "XE6zdSpOGaxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN:\n",
        "  def __init__(self,hidden_depth,num_neuron,num_input,num_output,activation=Sigmoid):\n",
        "    def init_var(i,o):\n",
        "      return np.random.normal(0.0,0.01,(i,o)),np.zeros((o,))\n",
        "\n",
        "    self.sequence = list()\n",
        "    # 첫 은닉층\n",
        "    W,b = init_var(num_input,num_neuron)\n",
        "    self.sequence.append(Neuron(W,b,activation))\n",
        "\n",
        "    # 은닉층들\n",
        "    for _ in range(hidden_depth):\n",
        "      W,b = init_var(num_neuron,num_neuron)\n",
        "      self.sequence.append(Neuron(W,b,activation))\n",
        "    \n",
        "    # 출력층\n",
        "    W,b = init_var(num_neuron,num_output)\n",
        "    self.sequence.append(Neuron(W,b,activation))\n",
        "  \n",
        "  def __call__(self,x):\n",
        "    for layer in self.sequence:\n",
        "      x = layer(x)\n",
        "    return x \n",
        "\n",
        "  def calc_gradient(self,loss_obj):\n",
        "    loss_obj.dh = loss_obj.grad()\n",
        "    self.sequence.append(loss_obj)\n",
        "\n",
        "    # 역전파\n",
        "    for i in range(len(self.sequence)-1,0,-1):\n",
        "      l1 = self.sequence[i]\n",
        "      l0 = self.sequence[i-1] \n",
        "\n",
        "      l0.dh = _m(l0.grad(),l1.dh)\n",
        "      l0.dW = l0.grad_W(l1.dh)\n",
        "      l0_db = l0.grad_b(l1.dh)\n",
        "      \n",
        "    self.sequence.remove(loss_obj)"
      ],
      "metadata": {
        "id": "PWmpH82jGbcb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사하강 학습"
      ],
      "metadata": {
        "id": "ZKJWB6kmGb1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(network,x,y,loss_obj,alpha=0.01):\n",
        "  loss = loss_obj(network(x),y)\n",
        "  network.calc_gradient(loss_obj)\n",
        "  for layer in network.sequence:\n",
        "    layer.W += -alpha * layer.dW\n",
        "    layer.b += -alpha * layer.db\n",
        "  return loss"
      ],
      "metadata": {
        "id": "sBkMrg70GcnB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 동작"
      ],
      "metadata": {
        "id": "r6i6IBw9GdCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.normal(0.0,1.0,(10,))\n",
        "y = np.random.normal(0.0,1.0,(2,))\n",
        "\n",
        "dnn = DNN(hidden_depth=5,num_neuron=32,num_input=10,num_output=2,activation=Sigmoid)\n",
        "\n",
        "t = time.time()\n",
        "loss_obj = MeanSquaredError()\n",
        "for epoch in range(100):\n",
        "  loss = gradient_descent(dnn,x,y,loss_obj,0.01)\n",
        "  print('Epoch : {} Test loss : {}'.format(epoch,loss))\n",
        "\n",
        "print('{} seconds elapsed'.format(time.time()-t))"
      ],
      "metadata": {
        "id": "Nyh_ukuuGdeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e939f27e-bff3-4be0-f623-302fc00985bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 Test loss : 0.553041353570619\n",
            "Epoch : 1 Test loss : 0.5148961570724307\n",
            "Epoch : 2 Test loss : 0.4806355393274166\n",
            "Epoch : 3 Test loss : 0.4500353221625107\n",
            "Epoch : 4 Test loss : 0.42279893342894614\n",
            "Epoch : 5 Test loss : 0.3986022929360732\n",
            "Epoch : 6 Test loss : 0.37712191799174166\n",
            "Epoch : 7 Test loss : 0.35805098105461847\n",
            "Epoch : 8 Test loss : 0.3411073914604477\n",
            "Epoch : 9 Test loss : 0.32603691226589687\n",
            "Epoch : 10 Test loss : 0.3126133594105593\n",
            "Epoch : 11 Test loss : 0.3006372015125231\n",
            "Epoch : 12 Test loss : 0.289933375366072\n",
            "Epoch : 13 Test loss : 0.2803488027853616\n",
            "Epoch : 14 Test loss : 0.2717498862081238\n",
            "Epoch : 15 Test loss : 0.2640201322548207\n",
            "Epoch : 16 Test loss : 0.2570579753284309\n",
            "Epoch : 17 Test loss : 0.2507748281744773\n",
            "Epoch : 18 Test loss : 0.24509336080071772\n",
            "Epoch : 19 Test loss : 0.23994599554150717\n",
            "Epoch : 20 Test loss : 0.23527359953456234\n",
            "Epoch : 21 Test loss : 0.23102435349787875\n",
            "Epoch : 22 Test loss : 0.22715277564557668\n",
            "Epoch : 23 Test loss : 0.22361888077212377\n",
            "Epoch : 24 Test loss : 0.22038745631443055\n",
            "Epoch : 25 Test loss : 0.2174274391914422\n",
            "Epoch : 26 Test loss : 0.21471137920723762\n",
            "Epoch : 27 Test loss : 0.21221497667273348\n",
            "Epoch : 28 Test loss : 0.20991668359920185\n",
            "Epoch : 29 Test loss : 0.2077973593253158\n",
            "Epoch : 30 Test loss : 0.20583997275946864\n",
            "Epoch : 31 Test loss : 0.20402934456230537\n",
            "Epoch : 32 Test loss : 0.2023519235773276\n",
            "Epoch : 33 Test loss : 0.20079559265831148\n",
            "Epoch : 34 Test loss : 0.19934949975915456\n",
            "Epoch : 35 Test loss : 0.19800391076154247\n",
            "Epoch : 36 Test loss : 0.19675008103375158\n",
            "Epoch : 37 Test loss : 0.19558014315350175\n",
            "Epoch : 38 Test loss : 0.19448700860078652\n",
            "Epoch : 39 Test loss : 0.19346428154319623\n",
            "Epoch : 40 Test loss : 0.1925061831050539\n",
            "Epoch : 41 Test loss : 0.19160748474010664\n",
            "Epoch : 42 Test loss : 0.19076344952178725\n",
            "Epoch : 43 Test loss : 0.18996978033047363\n",
            "Epoch : 44 Test loss : 0.18922257405817983\n",
            "Epoch : 45 Test loss : 0.18851828107147436\n",
            "Epoch : 46 Test loss : 0.18785366927629207\n",
            "Epoch : 47 Test loss : 0.18722579221636398\n",
            "Epoch : 48 Test loss : 0.1866319607124659\n",
            "Epoch : 49 Test loss : 0.18606971761448576\n",
            "Epoch : 50 Test loss : 0.18553681529401642\n",
            "Epoch : 51 Test loss : 0.18503119555315192\n",
            "Epoch : 52 Test loss : 0.18455097166652976\n",
            "Epoch : 53 Test loss : 0.18409441230938847\n",
            "Epoch : 54 Test loss : 0.183659927155316\n",
            "Epoch : 55 Test loss : 0.18324605395413004\n",
            "Epoch : 56 Test loss : 0.18285144692355906\n",
            "Epoch : 57 Test loss : 0.18247486630856904\n",
            "Epoch : 58 Test loss : 0.18211516897974034\n",
            "Epoch : 59 Test loss : 0.181771299957392\n",
            "Epoch : 60 Test loss : 0.18144228476150442\n",
            "Epoch : 61 Test loss : 0.1811272224991499\n",
            "Epoch : 62 Test loss : 0.18082527961134814\n",
            "Epoch : 63 Test loss : 0.180535684210204\n",
            "Epoch : 64 Test loss : 0.18025772094502868\n",
            "Epoch : 65 Test loss : 0.17999072634303356\n",
            "Epoch : 66 Test loss : 0.17973408457625087\n",
            "Epoch : 67 Test loss : 0.17948722361166708\n",
            "Epoch : 68 Test loss : 0.17924961170626466\n",
            "Epoch : 69 Test loss : 0.1790207542128184\n",
            "Epoch : 70 Test loss : 0.17880019066596725\n",
            "Epoch : 71 Test loss : 0.17858749212132854\n",
            "Epoch : 72 Test loss : 0.17838225872330082\n",
            "Epoch : 73 Test loss : 0.17818411747975188\n",
            "Epoch : 74 Test loss : 0.17799272022405604\n",
            "Epoch : 75 Test loss : 0.17780774174695574\n",
            "Epoch : 76 Test loss : 0.1776288780825165\n",
            "Epoch : 77 Test loss : 0.17745584493403554\n",
            "Epoch : 78 Test loss : 0.17728837622718988\n",
            "Epoch : 79 Test loss : 0.17712622277897533\n",
            "Epoch : 80 Test loss : 0.176969151072124\n",
            "Epoch : 81 Test loss : 0.17681694212569488\n",
            "Epoch : 82 Test loss : 0.17666939045344637\n",
            "Epoch : 83 Test loss : 0.1765263031024028\n",
            "Epoch : 84 Test loss : 0.17638749876475987\n",
            "Epoch : 85 Test loss : 0.17625280695692305\n",
            "Epoch : 86 Test loss : 0.17612206726006047\n",
            "Epoch : 87 Test loss : 0.1759951286170754\n",
            "Epoch : 88 Test loss : 0.1758718486813791\n",
            "Epoch : 89 Test loss : 0.17575209321326796\n",
            "Epoch : 90 Test loss : 0.17563573552009348\n",
            "Epoch : 91 Test loss : 0.17552265593675687\n",
            "Epoch : 92 Test loss : 0.1754127413433733\n",
            "Epoch : 93 Test loss : 0.17530588471723058\n",
            "Epoch : 94 Test loss : 0.175201984716418\n",
            "Epoch : 95 Test loss : 0.17510094529273568\n",
            "Epoch : 96 Test loss : 0.17500267533169397\n",
            "Epoch : 97 Test loss : 0.1749070883176062\n",
            "Epoch : 98 Test loss : 0.1748141020219425\n",
            "Epoch : 99 Test loss : 0.17472363821326842\n",
            "0.36353278160095215 seconds elapsed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lc_WlP1ogOZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}