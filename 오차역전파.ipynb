{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM4AIKu8+dWt49D6Hk4svk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/ML-DLPratice/blob/main/%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 수치 미분을 이용한 심층 신경망 학습"
      ],
      "metadata": {
        "id": "roxmenm85KQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oOyTNluC5Ed5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility function"
      ],
      "metadata": {
        "id": "KH97xvGT5Tkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.0001\n",
        "\n",
        "def _t(x):\n",
        "  return np.transpose(x)\n",
        "\n",
        "def _m(A,B):\n",
        "  return np.matmul(A,B)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mean_squared_error(h,y):\n",
        "  return 1 / 2 * np.mean(np.square(h-y))"
      ],
      "metadata": {
        "id": "Iej9voij5TBT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 신경망 구현"
      ],
      "metadata": {
        "id": "XWbyqXFF5pNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "  def __init__(self,W,b,a):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.a = a \n",
        "\n",
        "    # 경사\n",
        "    self.dw = np.zeros_like(self.W)\n",
        "    self.db = np.zeros_like(self.b)\n",
        "  \n",
        "  def __call__(self,x):\n",
        "    return self.a(_m(_t(self.W),x)+self.b) # activation((W^T)x + b)"
      ],
      "metadata": {
        "id": "qgL0E5hV5ocC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 심층 신경망 구현"
      ],
      "metadata": {
        "id": "CFR7r-ZG6Vi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN:\n",
        "  def __init__(self,hidden_depth,num_neuron,num_input,num_output,activation=sigmoid):\n",
        "    def init_var(i,o):\n",
        "      return np.random.normal(0.0,0.01,(i,o)),np.zeros((o,))\n",
        "\n",
        "    self.sequence = list()\n",
        "    # 첫 은닉층\n",
        "    W,b = init_var(num_input,num_neuron)\n",
        "    self.sequence.append(Neuron(W,b,activation))\n",
        "\n",
        "    # 은닉층들\n",
        "    for _ in range(hidden_depth-1):\n",
        "      W,b = init_var(num_neuron,num_neuron)\n",
        "      self.sequence.append(Neuron(W,b,activation))\n",
        "    \n",
        "    # 출력층\n",
        "    W,b = init_var(num_neuron,num_output)\n",
        "    self.sequence.append(Neuron(W,b,activation))\n",
        "  \n",
        "  def __call__(self,x):\n",
        "    for layer in self.sequence:\n",
        "      x = layer(x)\n",
        "    return x \n",
        "\n",
        "  def calc_gradient(self,x,y,loss_func):\n",
        "    def get_new_sequence(layer_index,new_neuron):\n",
        "      new_sequence = list()\n",
        "      for i, layer in enumerate(self.sequence):\n",
        "        if i == layer_index:\n",
        "          new_sequence.append(new_neuron)\n",
        "        else:\n",
        "          new_sequence.append(layer)\n",
        "      return new_sequence\n",
        "\n",
        "    def eval_sequence(x,sequence):\n",
        "      for layer in sequence:\n",
        "        x = layer(x)\n",
        "      return x\n",
        "\n",
        "    loss = loss_func(self(x),y)\n",
        "\n",
        "    for layer_id, layer in enumerate(self.sequence): #iterate layer\n",
        "      for w_i,w in enumerate(layer.W): # iterate W(row)\n",
        "        for w_j, ww in enumerate(w): # iterate W (col)\n",
        "          W = np.copy(layer.W)\n",
        "          W[w_i][w_j] = ww + epsilon \n",
        "\n",
        "          new_neuron = Neuron(W,layer.b,layer.a)\n",
        "          new_seq = get_new_sequence(layer_id,new_neuron)\n",
        "          h = eval_sequence(x,new_seq) \n",
        "\n",
        "          num_grad = (loss_func(h,y)-loss) / epsilon #(f(x+eps)-f(x))/epsilon\n",
        "          layer.dw[w_i][w_j] = num_grad\n",
        "\n",
        "        for b_i, bb in enumerate(layer.b): # iterate b\n",
        "          b = np.copy(layer.b)\n",
        "          b[b_i] = bb + epsilon \n",
        "\n",
        "          new_neuron = Neuron(W,layer.b,layer.a)\n",
        "          new_seq = get_new_sequence(layer_id,new_neuron)\n",
        "          h = eval_sequence(x,new_seq) \n",
        "\n",
        "          num_grad = (loss_func(h,y)-loss) / epsilon #(f(x+eps)-f(x))/epsilon\n",
        "          layer.db[b_i] = num_grad\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "X7hypySx6Sgv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 경사하강법"
      ],
      "metadata": {
        "id": "oa3H3XSL-6Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(network,x,y,loss_obj,alpha=0.01):\n",
        "\n",
        "  loss = network.calc_gradient(x,y,loss_obj)\n",
        "  for layer in network.sequence:\n",
        "    layer.W += -alpha * layer.dw\n",
        "    layer.b += -alpha * layer.db\n",
        "  return loss"
      ],
      "metadata": {
        "id": "eXAd9gji-zRi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시험"
      ],
      "metadata": {
        "id": "XkxIYFFf_Nqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.normal(0.0,1.0,(10,))\n",
        "y = np.random.normal(0.0,1.0,(2,))\n",
        "\n",
        "dnn = DNN(hidden_depth=5,num_neuron=32,num_input=10,num_output=2,activation=sigmoid)\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "for epoch in range(100):\n",
        "  loss = gradient_descent(dnn,x,y,mean_squared_error,0.01)\n",
        "  print('Epoch : {} Test loss : {}'.format(epoch,loss))\n",
        "\n",
        "print('{} seconds elapsed'.format(time.time()-t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijts0Der_MOL",
        "outputId": "716ca8c6-18fd-4e16-979d-50d074d18ae4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 Test loss : 0.5342294150442677\n",
            "Epoch : 1 Test loss : 0.5314448482373073\n",
            "Epoch : 2 Test loss : 0.5286751831098883\n",
            "Epoch : 3 Test loss : 0.5259206902809807\n",
            "Epoch : 4 Test loss : 0.5231816302558056\n",
            "Epoch : 5 Test loss : 0.5204582533695035\n",
            "Epoch : 6 Test loss : 0.517750799746941\n",
            "Epoch : 7 Test loss : 0.5150594992780231\n",
            "Epoch : 8 Test loss : 0.5123845716086496\n",
            "Epoch : 9 Test loss : 0.5097262261463181\n",
            "Epoch : 10 Test loss : 0.5070846620805649\n",
            "Epoch : 11 Test loss : 0.5044600684172731\n",
            "Epoch : 12 Test loss : 0.5018526240265765\n",
            "Epoch : 13 Test loss : 0.4992624977041744\n",
            "Epoch : 14 Test loss : 0.4966898482446226\n",
            "Epoch : 15 Test loss : 0.4941348245273312\n",
            "Epoch : 16 Test loss : 0.49159756561339096\n",
            "Epoch : 17 Test loss : 0.48907820085371473\n",
            "Epoch : 18 Test loss : 0.48657685000737505\n",
            "Epoch : 19 Test loss : 0.48409362336936634\n",
            "Epoch : 20 Test loss : 0.4816286219078043\n",
            "Epoch : 21 Test loss : 0.47918193740962767\n",
            "Epoch : 22 Test loss : 0.4767536526338325\n",
            "Epoch : 23 Test loss : 0.4743438414725314\n",
            "Epoch : 24 Test loss : 0.47195256911830197\n",
            "Epoch : 25 Test loss : 0.4695798922379183\n",
            "Epoch : 26 Test loss : 0.46722585915151554\n",
            "Epoch : 27 Test loss : 0.4648905100169314\n",
            "Epoch : 28 Test loss : 0.46257387701815433\n",
            "Epoch : 29 Test loss : 0.46027598455804375\n",
            "Epoch : 30 Test loss : 0.4579968494541813\n",
            "Epoch : 31 Test loss : 0.4557364811375855\n",
            "Epoch : 32 Test loss : 0.45349488185397824\n",
            "Epoch : 33 Test loss : 0.45127204686674904\n",
            "Epoch : 34 Test loss : 0.4490679646614234\n",
            "Epoch : 35 Test loss : 0.44688261715113653\n",
            "Epoch : 36 Test loss : 0.4447159798826962\n",
            "Epoch : 37 Test loss : 0.4425680222427398\n",
            "Epoch : 38 Test loss : 0.44043870766375315\n",
            "Epoch : 39 Test loss : 0.43832799382947996\n",
            "Epoch : 40 Test loss : 0.4362358328794027\n",
            "Epoch : 41 Test loss : 0.4341621716121153\n",
            "Epoch : 42 Test loss : 0.43210695168692537\n",
            "Epoch : 43 Test loss : 0.43007010982383787\n",
            "Epoch : 44 Test loss : 0.42805157800141574\n",
            "Epoch : 45 Test loss : 0.42605128365226497\n",
            "Epoch : 46 Test loss : 0.4240691498560081\n",
            "Epoch : 47 Test loss : 0.4221050955295238\n",
            "Epoch : 48 Test loss : 0.4201590356141671\n",
            "Epoch : 49 Test loss : 0.4182308812600044\n",
            "Epoch : 50 Test loss : 0.41632054000663693\n",
            "Epoch : 51 Test loss : 0.41442791596070705\n",
            "Epoch : 52 Test loss : 0.4125529099698667\n",
            "Epoch : 53 Test loss : 0.410695419793124\n",
            "Epoch : 54 Test loss : 0.40885534026733256\n",
            "Epoch : 55 Test loss : 0.4070325634700824\n",
            "Epoch : 56 Test loss : 0.40522697887850956\n",
            "Epoch : 57 Test loss : 0.4034384735243476\n",
            "Epoch : 58 Test loss : 0.40166693214481614\n",
            "Epoch : 59 Test loss : 0.39991223732969644\n",
            "Epoch : 60 Test loss : 0.39817426966420255\n",
            "Epoch : 61 Test loss : 0.39645290786798293\n",
            "Epoch : 62 Test loss : 0.3947480289299077\n",
            "Epoch : 63 Test loss : 0.39305950823893315\n",
            "Epoch : 64 Test loss : 0.3913872197108881\n",
            "Epoch : 65 Test loss : 0.389731035911297\n",
            "Epoch : 66 Test loss : 0.3880908281742068\n",
            "Epoch : 67 Test loss : 0.38646646671703233\n",
            "Epoch : 68 Test loss : 0.3848578207515948\n",
            "Epoch : 69 Test loss : 0.38326475859121784\n",
            "Epoch : 70 Test loss : 0.38168714775407087\n",
            "Epoch : 71 Test loss : 0.3801248550627443\n",
            "Epoch : 72 Test loss : 0.3785777467401229\n",
            "Epoch : 73 Test loss : 0.3770456885016474\n",
            "Epoch : 74 Test loss : 0.3755285456440258\n",
            "Epoch : 75 Test loss : 0.3740261831304168\n",
            "Epoch : 76 Test loss : 0.3725384656721663\n",
            "Epoch : 77 Test loss : 0.3710652578073053\n",
            "Epoch : 78 Test loss : 0.3696064239755782\n",
            "Epoch : 79 Test loss : 0.3681618285904894\n",
            "Epoch : 80 Test loss : 0.36673133610801445\n",
            "Epoch : 81 Test loss : 0.3653148110924209\n",
            "Epoch : 82 Test loss : 0.3639121182790219\n",
            "Epoch : 83 Test loss : 0.3625231226340596\n",
            "Epoch : 84 Test loss : 0.36114768941180064\n",
            "Epoch : 85 Test loss : 0.3597856842088686\n",
            "Epoch : 86 Test loss : 0.3584369730158993\n",
            "Epoch : 87 Test loss : 0.3571014222666726\n",
            "Epoch : 88 Test loss : 0.35577889888472336\n",
            "Epoch : 89 Test loss : 0.35446927032746256\n",
            "Epoch : 90 Test loss : 0.3531724046280437\n",
            "Epoch : 91 Test loss : 0.35188817043491116\n",
            "Epoch : 92 Test loss : 0.3506164370491446\n",
            "Epoch : 93 Test loss : 0.349357074459681\n",
            "Epoch : 94 Test loss : 0.3481099533765005\n",
            "Epoch : 95 Test loss : 0.3468749452618312\n",
            "Epoch : 96 Test loss : 0.34565192235939285\n",
            "Epoch : 97 Test loss : 0.3444407577218759\n",
            "Epoch : 98 Test loss : 0.3432413252365756\n",
            "Epoch : 99 Test loss : 0.34205349964935655\n",
            "95.09700226783752 seconds elapsed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MGJF-JT7_Tqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}