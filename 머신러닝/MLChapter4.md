# 1. Introduction
- 신경망 학습 방법은 실제 값, 이산 값, vector 값 목표 함수의 근사치에 대한 접근 방식을 제공
- 데이터 해석 학습과 같은 유형의 문제에 대해 가장 효과적인 방법 중 하나

## 1. Biological Motivation
- 생물학점 학습 시스템이 상호 연결된 뉴런의 매우 복잡한 웹으로 구축되어 있다는 관찰에서 부분적으로 영감을 받음
- 인공신겨망도 조밀하게 상호 연결된 단위 집합으로 구축됨
- 각 단위는 다수의 실제 값 입력과 실제 값을 출력


## 2. Appropriate problems for neural network learning
- 역전파 알고리즘은 가장 일반적으로 사용되는 ANN 학습 기술
- 목표 함수 출력은 이산 값, 실수 값 또는 여러 실수 값 속성의 vector
- Network train algorithm > Decision Tree. 훈련 시간은 network의 가중치 수 고려된 훈련 예제의 수 및 다양한 학습 알고리즘 매개 변수의 설정과 같은 요인에 따라 몇 초에서 몇 시간까지 다양할 수 있음
- 학습된 목표 함수의 빠른 평가가 필요할 수 있음. ANN 학습 시간은 비교적 길지만, 학습된 신경망을 실제에 적용하기 위해 평가하는 것은 일반적으로 매우 빠름
- 학습된 신경망은 학습된 규칙보다 인간에게 쉽게 전달되지 않음

## 3. Perceptron
- ANN system의 한 유형은 perceptron이라는 단위를 기반으로 함
- Perceptron은 실제 값 입력 벡터를 취하고, 이러한 입력의 선형 조합을 계산한 다음, 결과가 일부 임계값보다 크면 1을 출력하고, 그렇지 않으면 -1을 출력

<img width="255" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189894789-42b053df-c723-4636-a93d-22847637556b.PNG">

- 입력 $x_1$부터 x까지 perceptron에 의해 계산되는 출력은 다음과 같음

<img width="239" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189895033-422bb060-c45a-40fa-9e37-1b8cca1e0d36.PNG">

<img width="210" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189895534-fa73a0c3-316f-461f-b2b4-9090f2748268.PNG">

- Perceptron 학습에서 고려된 후보 가설 공간 H는 가능한 모든 실제 값 가중치 vecor의 집합

<img width="90" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189896191-81585925-0cd6-4995-9730-5ab16a8f5539.PNG">

### 1. Representatinal Power of Perceptrons
- Perceptron을 n차원 공간에서 hyperplane에 나타나는 것

<img width="220" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189896529-58dbba7e-2827-4571-a11e-e9e2737eaf3d.PNG">

### 2. The Perceptron Training Rule
- Perceptron 각 주어진 학습 data에 대해 올바른 출력을 생성하도록 하는 가중치 vector를 결정
- Perceptron 규칙과 Delta 규칙이라는 두 가지 고려
- 가중치 vector를 학습하는 한 가지 방법은 무작위 가중치로 시작한 다음 각 학습 data에 perceptron을 반복적으로 적용하여 가중치를 수정
- 위의 과정이 반복되며, perceptron이 모든 학습 data를 올바르게 분류할 때까지 필요한 만큼 훈련 예제를 반복

<img width="80" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189898786-0d84f2d1-ca53-497d-b13c-a7eaf931b32a.PNG">

- t : 현재 학습 과정에서의 target output, o : output generated by the perceptron, $\eta$ : 학습율(각 단계에서 가중치가 바뀌는 정도를 조절, 일반적으로 작은 값으로 설정)
- 목표 출력이 +1일 때 perceptron이 -1을 출력한다고 가정하였을 때, 이 경우 perceptron 출력을 -1이 아닌 +1로 만들려면 가중치를 변경하여 $\vec{w} \cdot \vec{x}$의 값을 증가시켜야 함

## 3.Gradient Descent and the Delta Rule
- Perceptron 규칙은 학습 data가 선형적으로 분리될 수 있을 때 성공적인 가중치 vector를 찾지만, 학습 데이터가 선형적으로 분리되지 않으면 수렴하지 못할 수도 있음.
- Delta Rule은 위의 문제점을 해결하기 위해 등장
- Key idea : 경사하강법을 이용해서 가능한 가중치 vector의 가설 공간을 검색하여 학습 data에 적합한 가중치를 찾는 것
- 역전파의 기초
- 임계값이 없는 perceptron을 훈련시키는 작업
- 선형 단위로부터 가중치를 배우기 위해 training error를 측정하는 방법을 구체화
- 많은 error 측정 방법이 있지만, 가장 공통정인 방법은 아래와 같음

<img width="115" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189902841-258f2da6-9d25-4a9a-8b87-a7be9578b48d.PNG">

- D : training dataset, $t_d$ : training example의 target output, $o_d$ : linear unit의 output
- E는 $t_d$와 $o_d$ 차이의 제곱의 반이며, 모든 훈련 예제에 걸쳐 합계됨
- E는 $\vec{w}$의 함수 <- o는 가중치 vector에 의존

### 1. VISUALIZING THE HYPOTHESIS SPACE

<img width="355" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189903742-1d829bea-16d8-4784-9670-f7ac1cdf6628.PNG">

- 축 $w_o$와 $w_1$은 단순한 선형 단위의 두 가중치에 대해 가능한 값 -> 전체 가설 공간을 나타냄
- 수직축 : 일부 고정된 훈련과 관련된 오류 E
- E를 정의하기 위한 방법을 고려할때, 선형 단위의 경우 이 오류 표면은 항상 전역 최소값이 되어야 함
- 경사하강법은 임의의 초기 가중치 vector로 시작한 다음 작은 단계로 반복적으로 수정하여 E를 최소화하는 가중치 vector를 결정
- 위의 과정은 global minimum에 도달할 때까지 계속

### 2. DERIVATION OF THE GRADIENT DESCENT RULE
- 가장 가파른 하강 방향을 계산하기 위해서는 vector w의 각 성분에 대한 E의 도함수를 계산
- 이러한 vector 도함수를 vector w에 대한 E의 기울기라고 함.

<img width="133" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189904558-51d34359-b5b3-474d-85be-d51e52b5cb70.PNG">

- $\bigtriangledown E(\vec{w})$ 그 자체가 vector, 그 성분들은 각각의 원소에 대한 E의 부분 도함수
- 기울기는 E에서 가장 가파른 증가를 생성하는 방향을 지정 -> 이 vector의 음수는 가장 가파른 감소 방향 제시
- 경사하강 방법

![render](https://user-images.githubusercontent.com/80622859/189905670-3c58f6aa-b130-4db3-a957-36dd5995083c.png)

- $\eta$ : 학습 속도라고 불리는 양의 상수, 경사하강 단계에서 크기를 결정
- 가중치를 반복적으로 update하는 algorithm

<img width="150" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189905870-abf66f16-a2c0-48d8-b14a-d909ffb03e27.PNG">

- $\eta$가 작으면 algorithm은 훈련 예제가 선형적으로 분리 가능한지 여부에 관계없이 최소 오류가 있는 가중치로 수렴.
- $\eta$가 너무 크면 경사하강법은 오류 지표면에 도달하기 보다는 최소값을 통과할 가능성 존재. 이러하 이유로 경사하강법을 진행할수록 $\eta$의 값을 점진적으로 

### 3.STOCHASTIC APPROXIMATION TO GRADIENT DESCENT
- 경사하강법의 문제점
1. Global minimum으로 수렴하는 것이 때때로 상당히 느릴 수 있음
2. Local minimum이 여러 개 있는 경우 global minimum으로 도달하지 않을 수도 있음
- 확률적 경사 하강법 사용
- 기본 경사하강법은 D의 모든 훈련 예제를 합산한 후 가중치를 update
- 확률적 경사 하강법은 각 개별 에제에 대한 오류 계산에 따라 가중치를 점진적으로 update
- Random하게 추출한 일부 data를 사용(전체 data 사용 X)
- 학습 중간 과정에서 결과의 진폭이 크고 불안정하며, 속도가 매우 빠름, memory 소모량 낮음
- 지역 최솟값에 빠진다할지라도, 지역 최솟값에 쉽게 빠져나올 수 있음

## 4. MULTILAYER NETWORKS AND THE BACKPROPAGATION ALGORITHM
- 단일 퍼셉트론은 선형 결정 표면만을 표현 가능
- MLP는 다양한 비선형 결정 표면을 표현 가능

### 1. A Differentiable Threshold Unit
- Sigmoid 단위 perceptron

![render](https://user-images.githubusercontent.com/80622859/189907873-1b7b6f6d-42ee-49dc-be6a-15f0260aad1e.png)

- 출력 범위는 0과 1 사이로 입력과 함께 단조롭게 증가
- 매우 큰 입력 domain을 작은 범위의 출력에 mapping하기 때문에 squashing function이라고도 함
- Sigmoid 함수 대신 tanh 함수가 사용되기도 함

### 2. The Backpropagation Algorithm
- 역전파가 직면한 학습 문제는 network의 모든 unit에 대해 가능한 모든 가중치 값으로 정의된 큰 가설 공간을 검색하는 것
- 다층 네트워크의 경우에는 단일 최소 포물선 오류 표면과 달리 여러 개의 지역 최소값을 가질 수 있음

#### 역전파 1단계

<img width="291" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910521-52862ed2-f002-4dfb-b741-4642f77c7c11.PNG">

- 순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치 update
- 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 update하는 단계를 역전파 1단계, 그리고 N층과 N층의 이전층 사이의 가중치를 update하는 단계를 역전파 2단계

<img width="116" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189909303-2ba6b96b-b92a-4a88-a786-c5c0344a4db0.PNG">

- Weights we have to update : $w_5\,,w_6\,,w_7\,,w_8$(4개)
- For updatin $w_5$, we must calculate $\frac{\partial E_{total}}{\partial w_5}$
- 미분의 연쇄법칙

<img width="175" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910203-2bc0d63f-94c2-414e-baaa-62e542071121.PNG">

- $E_{total}$ : 순전파를 진행하고 계산했던 전체 오차값

<img width="337" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910367-c92f28e3-be9d-4ec1-a771-643e8b6b092c.PNG">

<img width="403" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910428-e4ce9f58-5c65-4972-9e81-1f0c6e7d8029.PNG">

- 두 번째 항
- Sigmoid 함수의 미분 = $f(x) \times (1-f(x))$

<img width="366" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189910801-d8a2fcb0-0072-44e5-9bc2-eb5cb754b5bd.PNG">

- 세번째 항

<img width="152" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911071-321ad747-dc5c-456a-a88c-9e9fb0adc201.PNG">

- 최종

<img width="346" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911480-b10746ed-95fc-413d-84c9-6376a7587c2c.PNG">

<img width="347" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911553-c94f336b-2cd7-4b7a-9fa4-925debed080d.PNG">

#### 역전파 2단계

<img width="297" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911692-833c97d1-0270-4058-93bf-c20828b9e031.PNG">

- $w_1$ update = get $\frac{\partial E_{total}}{\partial w_1}

<img width="184" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189911919-0a7bb19f-f17d-4f8c-8491-a72183cf0ff8.PNG">

<img width="145" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912041-b7dcd300-e16e-4793-9b30-9429dec30a29.PNG">

<img width="272" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912137-addc849b-9b9a-4d9e-a902-668704388e74.PNG">

<img width="324" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912265-e6117b18-7028-40cd-930b-38b3f5bdbea3.PNG">

- 첫번째 항을 구함
- 나머지 두 항

<img width="368" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912380-ea3789f8-86ea-44d1-b4fd-2fbf031ba2ab.PNG">

- 최종

<img width="310" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912475-e3b92a01-b147-4c00-b97b-ef43dd8aa313.PNG">

<img width="341" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189912533-1bf66dc8-efe1-466b-9b81-909aa3520b3b.PNG">





