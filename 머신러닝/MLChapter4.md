# 1. Introduction
- 신경망 학습 방법은 실제 값, 이산 값, vector 값 목표 함수의 근사치에 대한 접근 방식을 제공
- 데이터 해석 학습과 같은 유형의 문제에 대해 가장 효과적인 방법 중 하나

## 1. Biological Motivation
- 생물학점 학습 시스템이 상호 연결된 뉴런의 매우 복잡한 웹으로 구축되어 있다는 관찰에서 부분적으로 영감을 받음
- 인공신겨망도 조밀하게 상호 연결된 단위 집합으로 구축됨
- 각 단위는 다수의 실제 값 입력과 실제 값을 출력


## 2. Appropriate problems for neural network learning
- 역전파 알고리즘은 가장 일반적으로 사용되는 ANN 학습 기술
- 목표 함수 출력은 이산 값, 실수 값 또는 여러 실수 값 속성의 vector
- Network train algorithm > Decision Tree. 훈련 시간은 network의 가중치 수 고려된 훈련 예제의 수 및 다양한 학습 알고리즘 매개 변수의 설정과 같은 요인에 따라 몇 초에서 몇 시간까지 다양할 수 있음
- 학습된 목표 함수의 빠른 평가가 필요할 수 있음. ANN 학습 시간은 비교적 길지만, 학습된 신경망을 실제에 적용하기 위해 평가하는 것은 일반적으로 매우 빠름
- 학습된 신경망은 학습된 규칙보다 인간에게 쉽게 전달되지 않음

## 3. Perceptron
- ANN system의 한 유형은 perceptron이라는 단위를 기반으로 함
- Perceptron은 실제 값 입력 벡터를 취하고, 이러한 입력의 선형 조합을 계산한 다음, 결과가 일부 임계값보다 크면 1을 출력하고, 그렇지 않으면 -1을 출력

<img width="255" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189894789-42b053df-c723-4636-a93d-22847637556b.PNG">

- 입력 $x_1$부터 x까지 perceptron에 의해 계산되는 출력은 다음과 같음

<img width="239" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189895033-422bb060-c45a-40fa-9e37-1b8cca1e0d36.PNG">

<img width="210" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189895534-fa73a0c3-316f-461f-b2b4-9090f2748268.PNG">

- Perceptron 학습에서 고려된 후보 가설 공간 H는 가능한 모든 실제 값 가중치 vecor의 집합

<img width="90" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189896191-81585925-0cd6-4995-9730-5ab16a8f5539.PNG">

### 1. Representatinal Power of Perceptrons
- Perceptron을 n차원 공간에서 hyperplane에 나타나는 것

<img width="220" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189896529-58dbba7e-2827-4571-a11e-e9e2737eaf3d.PNG">

### 2. The Perceptron Training Rule
- Perceptron 각 주어진 학습 data에 대해 올바른 출력을 생성하도록 하는 가중치 vector를 결정
- Perceptron 규칙과 Delta 규칙이라는 두 가지 고려
- 가중치 vector를 학습하는 한 가지 방법은 무작위 가중치로 시작한 다음 각 학습 data에 perceptron을 반복적으로 적용하여 가중치를 수정
- 위의 과정이 반복되며, perceptron이 모든 학습 data를 올바르게 분류할 때까지 필요한 만큼 훈련 예제를 반복

<img width="80" alt="캡처" src="https://user-images.githubusercontent.com/80622859/189898786-0d84f2d1-ca53-497d-b13c-a7eaf931b32a.PNG">

- t : 현재 학습 과정에서의 target output, o : output generated by the perceptron, $\eta$ : 학습율(각 단계에서 가중치가 바뀌는 정도를 조절, 일반적으로 작은 값으로 설정)
- 목표 출력이 +1일 때 perceptron이 -1을 출력한다고 가정하였을 때, 이 경우 perceptron 출력을 -1이 아닌 +1로 만들려면 가중치를 변경하여 $\vec{w} \cdot \vec{x}$의 값을 증가시켜야 함

## 3.Gradient Descent and the Delta Rule
- Perceptron 규칙은 학습 data가 선형적으로 분리될 수 있을 때 성공적인 가중치 vector를 찾지만, 학습 데이터가 선형적으로 분리되지 않으면 수렴하지 못할 수도 있음.
- Delta Rule은 위의 문제점을 해결하기 위해 


