# 과대 적합(Overfitting)

![image](https://user-images.githubusercontent.com/80622859/225609310-ef7e0464-ac5c-4ece-916e-a9760e56a7d5.png)

- 마치 시험 족보를 외우듯이 주어진 문제의 답을 암기해버리는 것 
- 일반적인 특성을 확인 불가
- 검증 손실과 학습 손실 차이가 많이 나게 됨

# Dataset

- Training data(50%~70%) : 학습 과정에 보여지고, 실제 모형을 학습하는데 사용
- Validation data(5%~15%) : 학습 과정에 보여지지만, 모형 학습에 사용하지 않고 학습이 잘 되는지 검증하는데 사용
- Test data(30%~50%) : 학습 과정에서는 사용하지 않고, 학습을 마친 모형을 평가하기 위해 단 한 번만 사용
- 과적합을 막기 위해서 위처럼 나누는 것이 중요
- Training data에 대해서는 성능이 잘 나오나 validation data에서 성능이 안 좋을 시 과적합
- Training loop : Training set + Validation set

# 손실 함수 

![image](https://user-images.githubusercontent.com/80622859/225610425-75a8dc98-932d-4ca5-9bc9-b214cf23ba38.png)

- $Cost = Loss(Data|Model) + \lambda Complexity(Model)$
- 손실에 집중 : Training data에 대한 신뢰도가 높음. Training data에 속하지 않은 입력에 취약
- 복잡도를 낮춤 : 모형의 복잡도가 지나치게 높아지지 않도록 제약. Data training보다 일반화에 투자
- 

# 해결 방법

## 조기 종료(Early stopping)

![image](https://user-images.githubusercontent.com/80622859/225610671-cd25e022-4061-424a-9051-301557be7d60.png)

- 검증 손실이 여러 epoch 동안 감소하지 않으면 과적합으로 간주하여 학습 중단
- 유예(patience)를 두고 봄

## Drop out

![image](https://user-images.githubusercontent.com/80622859/225610952-9910b30b-2a58-4211-b848-37c9bf97770d.png)

- 지정한 비율의 node를 제거하고 학습하는 방법
- Test 시에는 모든 node를 사용하기 때문에, 여러 신경망을 ensemble한 효과를 가짐

## Batch normalization

![image](https://user-images.githubusercontent.com/80622859/225611382-b55e7b1b-4768-4978-95d8-ac63e723d72c.png)

- 중간 feature들을 그대로 사용하지 않고 변형하여 학습 

## L-2 Regularization

- $Complexity(Model) = \frac{1}{N}\sum_i \frac{1}{2} w^2_i = \frac{1}{2} ||w||^2$
- 아주 큰 가중치에 penalty
- 구불구불한 것보다 평평한 형태 선호
- Bayses' prior probability distribution(정규 분포)
- Ridge
- 가중치가 정규분포의 형태를 이루도록 함

### 사전 확률 분포(Prior Probability Distribution)

- 한 학급에 남자 학생이 30%, 여자 학생이 70%의 비율로 존재 -> 우리 학급에 어떤 학생은 0.7의 확신도로 여성이다
- 평균이 70이고 분산이 5.2인 영어 성적에 대한 정규 분포가 있을 때 -> 영어 시험의 반 평균이 70점 근방일 확률이 가장 높다
- a priori
- Data를 보기 전에 확률 분포를 예측하는 것
- Bayes’ rule

### L-2 정규화와 $\lambda$ 값

- $\lambda$ 값이 클수록 정규 분포에 가까움
- 0에 가까울수록 정규화가 이루어지지 않으며, 가중치는 평평한 분포를 지향

## L-1 Regularization

- $Complexity(Model) = \frac{1}{N} \sum_i |w_i| = ||w||_1$

- 가중치의 절대값에 penalty
- 값이 양수 또는 음수로 존재하면 줄이려 함
- 값이 희소해짐
- Bayes’ prior probability distribution(Laplace distribution)
- Lasso
- 가중치가 laplace distribution를 따르도록



![image](https://github.com/as9786/ML-DLPratice/assets/80622859/d3fad1c8-1aea-423d-894c-64a7d202b569)

## Lasso vs Ridge

![image](https://github.com/as9786/ML-DLPratice/assets/80622859/9d478584-fa4d-4869-8606-ca46fb583b65)

## MSE Loss

- $arg min_w \sum ||y_i – wx_i||^2_2$
- 오차가 클수록 더욱 큰 규제
- Data의 평균 – 존재하지 않는 값
- Data를 smoothing하는 효과
- 이상치에 취약 
- 평균을 나타내는 특성

## MAE Loss

- $arg min_w \sum |y_i – wx_i|
- 오차가 커져도 동일한 규제
- Data의 중간값 – 존재하는 정확한 값
- 존재하는 값을 사용하여 노멕한 특성
- 적게 존재하는 값을 무시하는 특성
- 중간값을 나타냄
- 이상치에 강건한 특성



