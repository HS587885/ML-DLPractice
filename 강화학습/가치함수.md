# 가치 함수(Value function)

## 보상(R, Reward)
- Agent의 행동에 대한 성공이나 실패를 측정하는 feedback
- Agent의 행동에 의해 평가된 보상은 즉시 주어질 수도, 지연될 수도 있음

## 할인율($\gamma$, Discount factor)
- 보통 0과 1 사이의 값으로 즉각적으로 주어지는 보상보다는 상대적으로 가치가 낮은 미래의 보상을 만들기 위해 고안
- $\gamma$가 0.8이고, 3 단계를 거쳐 10점의 보상을 받는다면 보상의 현재 가치는 $0.8^3 \times 10$

## Return(G, Total discount reward)
- 전체 보상을 시간에 따른 감가상각을 포함하여 합산
- $G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum^\infty_{k=0}\gamma^k R_{t+k+1}$

## 가치(Expected return starting from state s)
- 현재 상태 s에서부터 시작할 때 기대되는 반환(실제 경험을 통해서 받은 보상의 할인된 양) 값
- 가치 함수는 return의 기댓값이기 때문에 마치 주사위를 던져 보듯이 던져 보면서 기댓값을 구할 수 있음
- 계속 그 상태로부터 시작되거나 그 상태를 지나가는 episode를 시도해보면서 얻어진 보상들에 대한 data들로 그 가치 함수에 점점 다가갈 수 있음. 사실 주사위도 무한번 던져야 1/6이라는 실제 기댓값을 가지듯 이 가치 함수도 또한 무한히 시도해야봐야 실제 가치 함수를 찾을 수 있음

# Bellman Equation
- The value function can be decomposed into two parts
1. Immediate reward $R_{t+1}$
2. Discounted value of successor state $\gamma v(S_{t+1})$

![image](https://github.com/as9786/ML-DLPratice/assets/80622859/8b695526-9462-4df7-9db5-4bdc9c069073)

![image](https://github.com/as9786/ML-DLPratice/assets/80622859/8f5d4b6d-b407-4bc7-ae3c-d8d766dba5c0)

![image](https://github.com/as9786/ML-DLPratice/assets/80622859/293c70c7-7659-47bf-9fb3-fa89320f4f4c)

- 위에처럼 선형 방정식으로 표현 가능
- 상태가 적은 경우에 자주 사용
- 상태가 많을 경우 연산량이 높음

# Markov Reward Process

- Markov process의 각 상태에 보상을 추가하여 확장한 것
- <S,P,R,$\gamma$> - 4-tuple로 표현

## S
- 상태의 집합
- 상태는 바둑에서 바둑판에 돌리 어떻게 놓여저 있는가, 미로를 탈출하는 문제에서 현재의 위치를 나타냄

## P
- 각 요소가 $p(s'|s) = Pr(S_{t+1}=s'|S_t=s)$인 집합
- $p(s'|s)$는 현재 상태 s에서 s'으로 이동할 확률을 의미. Transition probability라고 함

## R
- 각 요소가 $r(s)=E[R_{t+1}|S_t=s]$ 인 집합
- r(s)는 state s에서 얻는 보상

## $\gamma$
- 즉각적으로 얻는 보상과 미래의 얻을 수 있는 보상 간의 중요도를 조절하는 변수
- 주로 0~1 사이의 값을 가짐. Discount factor

- 시간 복잡도 $O(n^2)$

# Markov Decision Process(MDP)

- Markov reward process에 action이라는 요소가 추가된 모형. <S,A,P,R,$\gamma$>라는 tuple로 정의

## Policy(정책, $\pi$)
- 각 상태 ($s \in S$)에 대해 Actions ($a \in A$)에 대한 확률 분포를 정의하는 함수
- $\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]$

## State Transition(P)
- MDP가 주어진 $\pi$를 따를 때, s에서 s'으로 이동할 확률
- $p_{\pi}(s'|s) = \sum_{a \in A} \pi(a|s)p(s'|s,a)$

## Reward(P)
- s에서 얻을 수 있는 보상
- $r_{\pi}(s) = \sum_{a \in A} \pi(a|s)r(s,a)$

## State-value function with policy
- MDP에서 state-value function v(s)는 Markov reward process의 staet-value function과 마찬가지로 state s에서 시작되었을 때, 얻을 수 있는 return의 기댓값을 의미
- 그러나 MDP는 주어진 정책에 따라 행동을 결정하고, 상태를 이동하기 때문에 MDP에서의 상태-가치 함수는 다음과 같음

![image](https://github.com/as9786/ML-DLPratice/assets/80622859/5b24ff85-4410-43ce-ac8f-ffac5104456f)

- 행동에 종속적이지 않음. 상태에 고정된 값
## Action-value function

-  $q_{\pi}(s,a)$는 상태 s에서 시작하여 a라는 행동을 취했을 때 얻을 수 있는 return의 기댓값

![image](https://github.com/as9786/ML-DLPratice/assets/80622859/4ba477f2-0179-4150-a450-6ea13f7c2e46)

- State-value function은 어떠한 상태가 더 많은 보상을 얻을 수 있는지, action-value function은 어떠한 상태에서 어떠한 행동을 취해야 더 많은 보상을 얻을 수 있는지 알려줌

## 최적의 상태-가치 함수와 최적의 행동-가치 함수

- The optimal state-value function $v_*(s)$ is the maximum value function over all policies
- $v_*(s) = max_{\pi}v_{\pi}(s)$
- The optimal action-value funtion $q_*(s,a)$ is the maximum action-value fuction over all policies
- $q_*(s,a) = max_{\pi}q_{\pi}(s,a)$
- The optimal value function specifies the best possible performance in the MDP
- An MDP is solved when we know the optimal value fn
- 최적의 함수를 찾는 것은 최적의 정책을 찾는 것과 같음
- Define a partial ordering over policies
- $\pi \geq \pi^{'} if V_{\pi}(s) \geq V_{\pi^{'}}(s), \forall s$
- For any Markov Decision process
- There exists an optimal policy $\pi_*$ that is better than or equal to all other policies, $\pi_* \geq \pi, \forall \pi$
- All optimal policies achieve the optimal value function, $V_{\pi_*}(s) = v_*(s)$
- All optimal policies achieve the optimal action-value function, $q_{\pi_*}(s,a) = q_*(s,a)$


