{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAvE22oVJb/7wcpGO08GU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/as9786/ML-DLPratice/blob/main/Pytorch/Pytorch(%EC%88%98%EC%A0%951).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "sekX2IlXD6iF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "uRTI7MkZDn6F"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "pts = []\n",
        "labels = []\n",
        "\n",
        "center_point = np.random.uniform(-8,8,(10,2))\n",
        "\n",
        "for label, value in enumerate(center_point):\n",
        "  for _ in range(100):\n",
        "    pts.append(value + np.random.randn(*value.shape))\n",
        "    labels.append(label)"
      ],
      "metadata": {
        "id": "4i-AMjuMD89y"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pts = np.array(pts)    \n",
        "labels = np.array(labels)  \n",
        "\n",
        "pts.shape, labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNJTYsV0EJtV",
        "outputId": "9f50973c-938f-4b12-c249-d3789bd4953b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 2), (1000,))"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "for idx in range(10):\n",
        "  mask = idx == labels\n",
        "  plt.scatter(pts[mask,0],pts[mask,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "99hmGZgdD91B",
        "outputId": "fa13bd24-3cbc-4ba3-f63f-bde636da140b"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To torch dataset"
      ],
      "metadata": {
        "id": "vJKmKN_kEZO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "ItWuGiCPEHo-"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M2gcEVduEeVH",
        "outputId": "1fa5d260-f2f2-4575-c3e7-1294bea7eadf"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.0+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  # 객체 생성자\n",
        "  def __init__(self,pts,labels):\n",
        "    self.X = pts\n",
        "    self.y = labels\n",
        "\n",
        "    # If length of X and y are not equal, raise error\n",
        "    assert len(self.X) == len(self.y)\n",
        "\n",
        "  # 길이 반환  \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    X = torch.tensor(self.X[idx],dtype=torch.float32)\n",
        "    y= self.y[idx]\n",
        "    return X,y\n"
      ],
      "metadata": {
        "id": "XOq0dPekEgmg"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "assert는 오류를 발생시키는 함수로 특정 조건이 어긋났을 경우 오류를 발생시킨다"
      ],
      "metadata": {
        "id": "E4xLKBtbFINP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch의 경우에는 모형 생성 시 가중치의 형식이 torch.float32이므로 독립 변수들도 같은 형식으로 맞추어주어야 함"
      ],
      "metadata": {
        "id": "277XdBkuFeCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### assert"
      ],
      "metadata": {
        "id": "aq1k8KvmFLqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert 1 == 1"
      ],
      "metadata": {
        "id": "0WoRgC1aEza5"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert 1==2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "AAqjazsdE0cJ",
        "outputId": "8c676bcf-810b-4196-9c13-7d1a885117a1"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-730332727407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Holdout"
      ],
      "metadata": {
        "id": "n-zRrnz2Fqca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7-aUE17vFNz4"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(pts,labels,test_size=0.3,random_state=0)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.1,random_state=0)"
      ],
      "metadata": {
        "id": "onX285-ZFvxX"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shapoe of X train : ',X_train.shape)\n",
        "print('Shapoe of X valid : ',X_valid.shape)\n",
        "print('Shapoe of X est : ',X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekxh_6cCF9yo",
        "outputId": "d27e1913-1237-4410-b95a-ad1bd789799d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapoe of X train :  (700, 2)\n",
            "Shapoe of X valid :  (270, 2)\n",
            "Shapoe of X est :  (30, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "valid_dataset = CustomDataset(X_valid, y_valid)\n",
        "test_dataset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "v1o3X-uXGO_A"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset\n",
        "print('Length of train dataset : ',len(train_dataset))\n",
        "print('Shape of X train : ' ,train_dataset.X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k5HapYvGZUt",
        "outputId": "4a4b3e56-86eb-4540-ca3a-0fa31c285f2d"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset :  700\n",
            "Shape of X train :  (700, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset,batch_size=64)\n",
        "test_loader = DataLoader(test_dataset,batch_size=32)"
      ],
      "metadata": {
        "id": "2sq0HofnGctn"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_BrkXyaHCvK",
        "outputId": "5ac52a06-2c36-41fd-c9c9-a32fc80460da"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 3.9547,  7.1368],\n",
              "         [ 1.4791,  8.3981],\n",
              "         [-7.2922,  6.3316],\n",
              "         [ 7.8890, -1.5535],\n",
              "         [ 0.6587, -0.7537],\n",
              "         [ 2.7752,  0.2845],\n",
              "         [ 1.9690,  3.7600],\n",
              "         [ 7.1773, -2.7431],\n",
              "         [ 8.1090, -1.0683],\n",
              "         [-0.0321,  1.7167],\n",
              "         [ 1.9247, -0.2750],\n",
              "         [ 1.5113,  7.3570],\n",
              "         [ 5.7656,  5.5967],\n",
              "         [ 1.3043,  3.2715],\n",
              "         [-6.5786, -6.9153],\n",
              "         [ 7.4308, -0.2957],\n",
              "         [ 3.5399,  6.5983],\n",
              "         [ 2.9443,  4.7796],\n",
              "         [ 4.6343, -1.0685],\n",
              "         [10.0130, -2.2690],\n",
              "         [-0.7748,  6.5980],\n",
              "         [-5.9769, -6.8781],\n",
              "         [ 7.8596, -1.6861],\n",
              "         [ 7.7435, -0.8678],\n",
              "         [ 7.5802, -0.3015],\n",
              "         [-2.9639,  1.0311],\n",
              "         [-1.3075,  2.5286],\n",
              "         [-0.2290,  7.3506],\n",
              "         [-1.6904,  0.1329],\n",
              "         [-1.5311,  6.9134],\n",
              "         [-8.6818,  5.4233],\n",
              "         [ 1.0036,  6.2452],\n",
              "         [ 0.1374,  1.2196],\n",
              "         [-1.5690,  1.7530],\n",
              "         [ 2.1681,  0.8066],\n",
              "         [ 0.6212,  5.9471],\n",
              "         [ 0.7564,  6.1354],\n",
              "         [ 1.9598,  3.2631],\n",
              "         [-5.9462, -6.6630],\n",
              "         [ 8.1743, -2.4041],\n",
              "         [ 2.3974,  6.7837],\n",
              "         [-0.1252,  2.1001],\n",
              "         [-6.2400, -5.7153],\n",
              "         [-6.5774, -5.6274],\n",
              "         [ 1.4865,  2.9749],\n",
              "         [-0.1634,  3.0330],\n",
              "         [-3.5101,  2.5858],\n",
              "         [-0.8006,  7.3503],\n",
              "         [ 3.0087,  0.0287],\n",
              "         [ 1.0156,  5.5122],\n",
              "         [ 0.6883,  0.3721],\n",
              "         [-1.6916,  2.1176],\n",
              "         [ 6.5669, -0.0422],\n",
              "         [ 5.0158,  1.1256],\n",
              "         [ 7.9475, -1.6188],\n",
              "         [ 5.2255,  6.3530],\n",
              "         [ 3.4598,  4.9175],\n",
              "         [-7.4492,  6.9362],\n",
              "         [ 3.1864, -0.8102],\n",
              "         [-3.0028,  6.6452],\n",
              "         [ 4.4724,  5.9627],\n",
              "         [-0.2755,  6.2930],\n",
              "         [ 1.2960,  7.1173],\n",
              "         [-1.4635,  3.8526]]),\n",
              " tensor([9, 6, 8, 4, 1, 5, 0, 4, 4, 0, 1, 6, 9, 0, 7, 4, 6, 0, 5, 4, 3, 7, 4, 4,\n",
              "         4, 2, 2, 6, 2, 3, 8, 6, 0, 2, 1, 6, 3, 0, 7, 4, 6, 2, 7, 7, 1, 0, 2, 3,\n",
              "         1, 6, 1, 2, 4, 5, 4, 9, 9, 8, 5, 3, 9, 3, 6, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iter는 반복 객체를 만들어주는 함수. iter 함수 사용 시 next 구문으로 data 확인 가능"
      ],
      "metadata": {
        "id": "3DwXvdkqHNVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_loader:\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OLzl-AuHDwS",
        "outputId": "a89ccd16-8ec1-4669-9545-c1046f355040"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모형"
      ],
      "metadata": {
        "id": "dVxNg7mVHStn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "d3YRwVXmHMrh"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    #super().__init__()\n",
        "    super(MyModel,self).__init__() # nn.Moudled의 속성을 생성자 변수들이 모두 가질 수 있도록 설정\n",
        "    self.linear = nn.Linear(2,50)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.out = nn.Linear(50,10)\n",
        "\n",
        "    # torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear(x)\n",
        "    x = self.relu(x)\n",
        "    output = self.out(x)\n",
        "    return output"
      ],
      "metadata": {
        "id": "y4BiF_1KHfqJ"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용가능한 장치 확인\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T_OnoUTIL2G",
        "outputId": "01858490-4526-4d20-fc24-b19d5f151cf5"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "True 일 경우 GPU를 사용할 수 있다는 뜻이고, False면 cpu로 연산을 수행한다는 뜻"
      ],
      "metadata": {
        "id": "exiX0QVjITG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set torch device"
      ],
      "metadata": {
        "id": "4Haf0DRaIZBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available() == True:\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U9Xr7ghwIbJs",
        "outputId": "09e3a48d-74a2-4054-b4e4-2cf52658fc10"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcybzauwIXQj",
        "outputId": "cee74b9a-3449-4714-8059-6baf655735bb"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel().to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX9ikBPtIDVy",
        "outputId": "38b64ca3-2351-4441-88ef-924b479fbb0e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (linear): Linear(in_features=2, out_features=50, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (out): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모형 요약"
      ],
      "metadata": {
        "id": "0N4Wm2kTJI-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yYWCE6PIpB4",
        "outputId": "5e8c1310-ac2f-41c4-f4a4-b6ceb7590faf"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmi2CP2kJIDk",
        "outputId": "12d5914c-cc76-42d3-e9d6-3869ca435842"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "MyModel                                  --\n",
              "├─Linear: 1-1                            150\n",
              "├─ReLU: 1-2                              --\n",
              "├─Linear: 1-3                            510\n",
              "=================================================================\n",
              "Total params: 660\n",
              "Trainable params: 660\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#손실 함수 및 최적화 함수\n",
        "optimizer = optim.Adam(model.parameters(),lr = 0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "xlPQVm2kJM7h"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습"
      ],
      "metadata": {
        "id": "iLMVAPIsKJfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader.dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsMovmDjKgdO",
        "outputId": "cd37f211-b2fe-4992-ee7a-33c223d97e30"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "700"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for X,y in train_loader:\n",
        "  sample_X = X\n",
        "  sample_y = y\n",
        "  break"
      ],
      "metadata": {
        "id": "fxQKwXAWK_zg"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_X = X.to(device)\n",
        "sample_y = y.to(device)"
      ],
      "metadata": {
        "id": "msSDdhI0LEYY"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(sample_X)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpQ2b3jLLH8b",
        "outputId": "0b49ec4f-58ed-4a72-fd4e-5c03e06a9faa"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-6.6177e-01, -1.5426e+00,  3.6124e-02,  1.7739e+00, -4.2248e-01,\n",
              "         -2.3030e+00,  3.8797e-01,  9.5968e-01,  6.0918e-01, -2.0350e-02],\n",
              "        [-2.1998e-01,  7.5619e-01,  1.8532e-01,  2.3148e-01,  1.2391e-01,\n",
              "         -2.5270e-01, -4.7157e-02, -1.0804e-02,  5.1116e-01, -5.9840e-01],\n",
              "        [-3.7862e-01,  1.6580e-01,  5.7216e-01,  8.4558e-01, -6.8644e-01,\n",
              "         -9.5728e-01, -2.1891e-01, -6.8417e-01,  1.6955e+00,  6.8548e-01],\n",
              "        [-1.0221e+00, -6.2085e-01, -9.7405e-01, -9.0543e-01,  2.4447e-01,\n",
              "         -2.8434e-01,  5.6707e-01,  2.2825e+00, -2.1679e+00, -2.6754e-01],\n",
              "        [-3.6199e-01,  6.0989e-01,  5.5936e-01,  3.0351e-01, -1.1692e-01,\n",
              "         -6.9643e-02, -2.4913e-01, -5.1767e-01,  7.4974e-01,  8.2169e-03],\n",
              "        [-5.8886e-01, -1.2948e+00,  5.2014e-02,  1.5871e+00, -3.7618e-01,\n",
              "         -2.0425e+00,  3.0330e-01,  7.2969e-01,  7.4585e-01,  1.8860e-02],\n",
              "        [-1.0354e+00, -4.2319e-01, -1.1348e+00, -1.1444e+00,  2.7476e-01,\n",
              "         -1.4159e-01,  7.1948e-01,  2.5670e+00, -2.3606e+00, -2.8968e-01],\n",
              "        [-3.9716e-01, -6.5360e-01,  6.6090e-02,  8.9039e-01, -1.9376e-01,\n",
              "         -1.1920e+00,  1.2313e-01,  3.1515e-01,  6.6398e-01,  1.6783e-02],\n",
              "        [-3.3861e-01,  1.4824e+00,  1.5956e+00,  9.4213e-01, -6.4190e-01,\n",
              "          2.3186e-01, -7.8500e-01, -1.4787e+00,  1.9145e+00,  5.5509e-01],\n",
              "        [-8.1209e-01, -1.7926e+00,  6.5221e-02,  2.3712e+00, -7.0213e-01,\n",
              "         -2.9700e+00,  4.3024e-01,  9.3062e-01,  1.2701e+00,  1.4646e-01],\n",
              "        [-5.2641e-01, -1.1674e-01,  5.0203e-01,  1.4619e+00, -9.4151e-01,\n",
              "         -1.5066e+00, -1.2978e-01, -5.6234e-01,  2.1766e+00,  8.3980e-01],\n",
              "        [-2.8612e-01,  1.5265e-01,  1.9046e-02,  1.4867e-01,  1.7240e-01,\n",
              "         -2.8189e-01, -4.1883e-02,  1.8549e-01,  1.5797e-01, -2.6783e-01],\n",
              "        [ 2.3541e-02,  2.6247e+00,  1.6068e-01,  5.2998e-01,  1.4183e-01,\n",
              "         -5.9750e-01, -1.5747e-01,  1.4076e-01,  8.3197e-01, -1.4817e+00],\n",
              "        [-3.7831e-01,  5.5894e-01,  9.9517e-01,  9.2402e-01, -7.7934e-01,\n",
              "         -7.1693e-01, -4.1037e-01, -1.0693e+00,  2.0166e+00,  8.8067e-01],\n",
              "        [-4.1411e-01,  9.1207e-01,  1.4435e+00,  1.1565e+00, -9.5877e-01,\n",
              "         -6.0649e-01, -6.0787e-01, -1.4718e+00,  2.5034e+00,  1.1284e+00],\n",
              "        [-6.8276e-01,  2.5383e+00,  1.3120e+00,  6.1267e-01, -1.4997e-01,\n",
              "          8.0037e-01, -8.6235e-01, -1.2545e+00,  1.2542e+00,  8.0722e-03],\n",
              "        [-6.7283e-01,  2.5011e+00,  1.3946e+00,  6.9829e-01, -2.2891e-01,\n",
              "          8.5950e-01, -8.6461e-01, -1.3495e+00,  1.3503e+00,  6.4028e-02],\n",
              "        [-8.2863e-01, -1.9836e+00, -4.2145e-02,  1.6274e+00, -4.6242e-01,\n",
              "         -2.4610e+00,  5.8656e-01,  1.6877e+00, -4.5164e-01, -2.6219e-01],\n",
              "        [-1.1256e+00, -4.7570e-01, -1.2630e+00, -1.2722e+00,  2.8607e-01,\n",
              "         -1.4521e-01,  8.0425e-01,  2.8201e+00, -2.6072e+00, -2.8838e-01],\n",
              "        [-3.8024e-01,  7.2300e-01,  6.6389e-01,  3.5204e-01, -1.4074e-01,\n",
              "         -2.9554e-03, -3.1222e-01, -6.2092e-01,  8.3661e-01,  2.1380e-02],\n",
              "        [-4.1575e-01,  8.8814e-01,  7.3882e-01,  3.7754e-01, -1.3509e-01,\n",
              "          1.0628e-01, -3.6377e-01, -6.9228e-01,  8.6886e-01, -1.3244e-02],\n",
              "        [-3.9015e-01,  3.5970e-01,  7.8254e-01,  9.1371e-01, -7.6274e-01,\n",
              "         -8.8460e-01, -3.2414e-01, -9.0465e-01,  1.9330e+00,  8.3250e-01],\n",
              "        [-5.8980e-01,  2.0211e+00,  1.2814e+00,  7.0613e-01, -2.7238e-01,\n",
              "          7.3907e-01, -7.0760e-01, -1.2374e+00,  1.2824e+00,  6.7228e-02],\n",
              "        [-3.4841e-01, -1.3169e-01,  2.6034e-01,  8.6685e-01, -4.9037e-01,\n",
              "         -9.8298e-01, -4.3597e-02, -2.9082e-01,  1.1957e+00,  4.5083e-01],\n",
              "        [-6.0416e-01, -1.3259e+00,  5.3770e-02,  1.6433e+00, -4.0152e-01,\n",
              "         -2.1083e+00,  3.1069e-01,  7.3806e-01,  7.9301e-01,  3.0953e-02],\n",
              "        [ 3.1079e-01,  4.4643e+00, -1.2052e-02,  8.4761e-01,  1.6369e-01,\n",
              "         -1.1073e+00, -1.5405e-01,  4.8168e-01,  1.2434e+00, -2.4149e+00],\n",
              "        [-7.4882e-01,  2.9841e+00,  1.6575e+00,  8.2842e-01, -2.9653e-01,\n",
              "          1.0805e+00, -1.0417e+00, -1.6168e+00,  1.5760e+00,  1.5614e-01],\n",
              "        [-2.7309e-01,  2.6153e-01,  4.6971e-01,  4.5157e-01, -3.8729e-01,\n",
              "         -5.5857e-01, -1.5066e-01, -5.1632e-01,  1.0896e+00,  3.9300e-01],\n",
              "        [-1.1856e+00, -7.6689e-01, -1.1506e+00, -1.0592e+00,  2.6340e-01,\n",
              "         -3.4282e-01,  6.6815e-01,  2.6421e+00, -2.5415e+00, -2.7319e-01],\n",
              "        [-7.0624e-01, -1.4026e+00,  7.7535e-02,  1.9768e+00, -6.1288e-01,\n",
              "         -2.4838e+00,  3.1901e-01,  6.6304e-01,  1.2764e+00,  1.6050e-01],\n",
              "        [-2.8939e-01, -8.1976e-02,  2.0405e-01,  6.0277e-01, -3.1199e-01,\n",
              "         -7.1876e-01, -4.5609e-02, -2.3081e-01,  8.3925e-01,  3.2053e-01],\n",
              "        [-6.9750e-01, -1.5854e+00,  4.7393e-02,  1.9441e+00, -4.9601e-01,\n",
              "         -2.4743e+00,  3.8431e-01,  9.0308e-01,  8.5441e-01,  3.7391e-02],\n",
              "        [-6.6155e-01, -9.1136e-01,  1.5244e-01,  1.7770e+00, -7.5792e-01,\n",
              "         -2.1224e+00,  2.0432e-01,  1.9461e-01,  1.7610e+00,  3.5658e-01],\n",
              "        [-3.8522e-01,  1.6146e-01,  5.8182e-01,  8.7192e-01, -7.0420e-01,\n",
              "         -9.7948e-01, -2.2290e-01, -6.9367e-01,  1.7313e+00,  6.9925e-01],\n",
              "        [-2.8400e-01,  5.8161e-01,  6.6181e-01,  4.3299e-01, -3.0817e-01,\n",
              "         -2.0987e-01, -2.7400e-01, -6.2459e-01,  9.9051e-01,  2.4147e-01],\n",
              "        [-6.8337e-01,  2.5368e+00,  1.2901e+00,  5.9264e-01, -1.3249e-01,\n",
              "          7.8407e-01, -8.5854e-01, -1.2299e+00,  1.2302e+00, -5.1450e-03],\n",
              "        [ 1.8671e-01,  3.5690e+00,  5.8766e-02,  7.0041e-01,  1.4882e-01,\n",
              "         -8.8438e-01, -1.4356e-01,  3.3647e-01,  1.0427e+00, -1.9790e+00],\n",
              "        [-3.6009e-01,  1.5506e+00,  1.6127e+00,  9.2470e-01, -6.1354e-01,\n",
              "          3.1819e-01, -7.9718e-01, -1.4970e+00,  1.8749e+00,  4.9479e-01],\n",
              "        [-6.7649e-02,  3.3667e+00,  3.9873e-01,  5.9043e-01,  1.4562e-01,\n",
              "         -4.4211e-01, -3.6504e-01, -1.2582e-01,  9.5172e-01, -1.6133e+00],\n",
              "        [-1.1515e+00, -7.6478e-01, -1.0915e+00, -9.9548e-01,  2.5567e-01,\n",
              "         -3.5264e-01,  6.2678e-01,  2.5306e+00, -2.4386e+00, -2.6966e-01],\n",
              "        [-2.8994e-01, -4.2000e-01,  6.6002e-02,  4.4094e-01,  8.1148e-03,\n",
              "         -6.9528e-01,  5.9190e-02,  2.6226e-01,  2.2614e-01, -7.7124e-02],\n",
              "        [-1.0245e-01,  1.4005e+00,  1.0359e-01,  3.8004e-01,  1.4794e-01,\n",
              "         -4.2085e-01, -5.2685e-02,  1.4016e-01,  6.1032e-01, -9.6421e-01],\n",
              "        [-1.8314e-01,  7.8048e-01,  1.4178e-01,  2.9078e-01,  1.3525e-01,\n",
              "         -2.9853e-01, -2.3732e-02,  8.0109e-02,  4.9383e-01, -6.3300e-01],\n",
              "        [ 4.5189e-01,  4.8745e+00, -3.4052e-01,  9.0344e-01,  2.4708e-01,\n",
              "         -1.4024e+00, -4.8433e-03,  8.8533e-01,  1.4243e+00, -2.7216e+00],\n",
              "        [-3.1493e-01,  3.9648e-01,  4.0407e-01,  2.6207e-01, -1.4211e-01,\n",
              "         -2.4998e-01, -1.5116e-01, -3.8725e-01,  6.6948e-01,  5.9618e-02],\n",
              "        [-4.1179e-01,  8.5166e-01,  7.3429e-01,  3.7964e-01, -1.3850e-01,\n",
              "          8.5248e-02, -3.5991e-01, -6.9177e-01,  8.7274e-01,  3.6173e-05],\n",
              "        [-5.1334e-01,  1.5190e-01, -9.4765e-01, -9.5374e-01,  7.1556e-02,\n",
              "         -8.8606e-02,  7.5606e-01,  2.1506e+00, -1.4799e+00, -5.5236e-01],\n",
              "        [-1.1263e+00, -7.0995e-01, -1.0896e+00, -1.0078e+00,  2.5706e-01,\n",
              "         -3.1853e-01,  6.3431e-01,  2.5168e+00, -2.4094e+00, -2.7147e-01],\n",
              "        [-7.4573e-01,  4.2016e+00,  1.2359e+00,  5.0074e-01,  1.6985e-01,\n",
              "          7.4141e-01, -1.1856e+00, -1.0783e+00,  1.2445e+00, -5.4709e-01],\n",
              "        [-3.9512e-01,  1.7110e-01,  6.0418e-01,  8.9690e-01, -7.2639e-01,\n",
              "         -9.9657e-01, -2.3373e-01, -7.2105e-01,  1.7844e+00,  7.2101e-01],\n",
              "        [-3.6443e-01,  4.5974e-01,  8.5760e-01,  8.3912e-01, -7.1178e-01,\n",
              "         -7.3082e-01, -3.4794e-01, -9.4156e-01,  1.8444e+00,  7.8989e-01],\n",
              "        [ 5.6577e-01,  5.7776e+00, -6.7971e-01,  1.0099e+00,  3.0485e-01,\n",
              "         -1.7239e+00,  5.5550e-02,  1.2105e+00,  1.7710e+00, -3.2285e+00],\n",
              "        [-7.5531e-01, -1.3462e+00,  1.0007e-01,  2.1225e+00, -7.5353e-01,\n",
              "         -2.6352e+00,  2.9216e-01,  5.2315e-01,  1.6614e+00,  2.6738e-01],\n",
              "        [-1.3984e-01,  2.4714e+00,  3.6187e-01,  4.4829e-01,  1.3474e-01,\n",
              "         -3.5334e-01, -2.6283e-01, -1.1934e-01,  7.8885e-01, -1.2412e+00],\n",
              "        [-9.9491e-01, -2.9948e-01, -1.1909e+00, -1.2848e+00,  2.5950e-01,\n",
              "         -5.8030e-02,  8.1877e-01,  2.6756e+00, -2.3645e+00, -3.4115e-01],\n",
              "        [-3.2213e-01,  4.4506e-01,  7.5622e-01,  6.5759e-01, -5.6162e-01,\n",
              "         -5.8622e-01, -2.8291e-01, -8.0381e-01,  1.5207e+00,  6.1181e-01],\n",
              "        [-3.0959e-01,  1.6258e-01,  1.5579e-01,  2.6891e-01, -2.3062e-02,\n",
              "         -3.2406e-01, -4.8464e-02, -1.6573e-01,  4.0220e-01,  5.0914e-02],\n",
              "        [-4.2563e-01,  1.0007e+00,  8.6068e-01,  4.6124e-01, -1.9365e-01,\n",
              "          1.7002e-01, -4.2327e-01, -8.1153e-01,  9.9379e-01,  5.0740e-02],\n",
              "        [-1.1502e+00, -4.7681e-01, -1.3100e+00, -1.3319e+00,  2.8924e-01,\n",
              "         -1.3269e-01,  8.4202e-01,  2.9130e+00, -2.6881e+00, -2.9264e-01],\n",
              "        [-4.4747e-01,  2.4134e-01,  7.7451e-01,  1.1225e+00, -8.9267e-01,\n",
              "         -1.1248e+00, -3.1564e-01, -9.0209e-01,  2.1686e+00,  9.0534e-01],\n",
              "        [-7.3538e-01, -1.5332e+00,  7.1375e-02,  2.0890e+00, -6.2771e-01,\n",
              "         -2.6249e+00,  3.5732e-01,  7.6259e-01,  1.2366e+00,  1.4552e-01],\n",
              "        [-1.5671e-01,  9.1412e-01,  7.6252e-02,  3.5600e-01,  1.3870e-01,\n",
              "         -3.5966e-01, -1.4501e-02,  1.7568e-01,  5.0891e-01, -7.5062e-01],\n",
              "        [ 2.4376e-01,  4.9862e+00,  1.7985e-01,  8.9430e-01,  1.5846e-01,\n",
              "         -9.8398e-01, -3.1383e-01,  2.6614e-01,  1.3173e+00, -2.5032e+00],\n",
              "        [-6.6825e-01,  2.5024e+00,  1.1034e+00,  4.6407e-01, -1.4194e-02,\n",
              "          6.4305e-01, -8.0722e-01, -1.0132e+00,  1.0540e+00, -1.3188e-01]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABey0OqSLSTM",
        "outputId": "f19a48e4-d1b1-4ef1-efaf-bfc34eec56d2"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.6618, -1.5426,  0.0361,  1.7739, -0.4225, -2.3030,  0.3880,  0.9597,\n",
              "         0.6092, -0.0204], device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion(output,sample_y).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijvMhEFYLnw-",
        "outputId": "5e005152-8086-4be6-9d12-04d5315c2159"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3240180015563965"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, loss_fn,epoch):\n",
        "  # model.train()\n",
        "  print(f'Epoch : {epoch}')\n",
        "  size = len(dataloader.dataset)\n",
        "  total_batch = len(dataloader)\n",
        "  running_loss = 0\n",
        "  for batch, (X, y) in enumerate(dataloader): \n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    output = model(X)\n",
        "    loss = loss_fn(output, y)\n",
        "\n",
        "    optimizer.zero_grad() # 이전 경사 값들의 정보를 날림\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Train loss : {loss.item():>7f} [{batch*len(X)}/{size}]')\n",
        "    running_loss += loss.item()\n",
        "  print(f'Average Train loss : {running_loss/total_batch}\\n')\n",
        "  return running_loss"
      ],
      "metadata": {
        "id": "wA6hrMlDJ_IU"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  train(model,train_loader,optimizer,criterion,epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptCWwv8_O9nE",
        "outputId": "4c7d7142-6e81-4864-bc81-c787c735d7f8"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "Train loss : 2.756764 [0/700]\n",
            "Train loss : 2.808532 [64/700]\n",
            "Train loss : 2.484109 [128/700]\n",
            "Train loss : 2.498271 [192/700]\n",
            "Train loss : 2.563560 [256/700]\n",
            "Train loss : 2.681324 [320/700]\n",
            "Train loss : 2.572083 [384/700]\n",
            "Train loss : 2.705210 [448/700]\n",
            "Train loss : 2.516575 [512/700]\n",
            "Train loss : 2.723630 [576/700]\n",
            "Train loss : 2.738921 [600/700]\n",
            "Average Train loss : 2.6408162117004395\n",
            "\n",
            "Epoch : 1\n",
            "Train loss : 2.759784 [0/700]\n",
            "Train loss : 2.444773 [64/700]\n",
            "Train loss : 2.472838 [128/700]\n",
            "Train loss : 2.913460 [192/700]\n",
            "Train loss : 2.568021 [256/700]\n",
            "Train loss : 2.405498 [320/700]\n",
            "Train loss : 2.670902 [384/700]\n",
            "Train loss : 2.676382 [448/700]\n",
            "Train loss : 2.454322 [512/700]\n",
            "Train loss : 2.511598 [576/700]\n",
            "Train loss : 2.419647 [600/700]\n",
            "Average Train loss : 2.5724749131636186\n",
            "\n",
            "Epoch : 2\n",
            "Train loss : 2.498538 [0/700]\n",
            "Train loss : 2.303638 [64/700]\n",
            "Train loss : 2.500078 [128/700]\n",
            "Train loss : 2.724443 [192/700]\n",
            "Train loss : 2.507537 [256/700]\n",
            "Train loss : 2.328057 [320/700]\n",
            "Train loss : 2.535163 [384/700]\n",
            "Train loss : 2.644997 [448/700]\n",
            "Train loss : 2.613237 [512/700]\n",
            "Train loss : 2.400087 [576/700]\n",
            "Train loss : 2.534881 [600/700]\n",
            "Average Train loss : 2.5082415017214688\n",
            "\n",
            "Epoch : 3\n",
            "Train loss : 2.402119 [0/700]\n",
            "Train loss : 2.581230 [64/700]\n",
            "Train loss : 2.476624 [128/700]\n",
            "Train loss : 2.580174 [192/700]\n",
            "Train loss : 2.534092 [256/700]\n",
            "Train loss : 2.549265 [320/700]\n",
            "Train loss : 2.252464 [384/700]\n",
            "Train loss : 2.471557 [448/700]\n",
            "Train loss : 2.319160 [512/700]\n",
            "Train loss : 2.340162 [576/700]\n",
            "Train loss : 2.401631 [600/700]\n",
            "Average Train loss : 2.44622523134405\n",
            "\n",
            "Epoch : 4\n",
            "Train loss : 2.621445 [0/700]\n",
            "Train loss : 2.453509 [64/700]\n",
            "Train loss : 2.171026 [128/700]\n",
            "Train loss : 2.381595 [192/700]\n",
            "Train loss : 2.531450 [256/700]\n",
            "Train loss : 2.276394 [320/700]\n",
            "Train loss : 2.240243 [384/700]\n",
            "Train loss : 2.330146 [448/700]\n",
            "Train loss : 2.410403 [512/700]\n",
            "Train loss : 2.485249 [576/700]\n",
            "Train loss : 2.345808 [600/700]\n",
            "Average Train loss : 2.3861154209483755\n",
            "\n",
            "Epoch : 5\n",
            "Train loss : 2.322407 [0/700]\n",
            "Train loss : 2.372706 [64/700]\n",
            "Train loss : 2.394376 [128/700]\n",
            "Train loss : 2.166963 [192/700]\n",
            "Train loss : 2.508661 [256/700]\n",
            "Train loss : 2.411947 [320/700]\n",
            "Train loss : 2.111403 [384/700]\n",
            "Train loss : 2.427731 [448/700]\n",
            "Train loss : 2.267295 [512/700]\n",
            "Train loss : 2.281913 [576/700]\n",
            "Train loss : 2.351149 [600/700]\n",
            "Average Train loss : 2.3287773565812544\n",
            "\n",
            "Epoch : 6\n",
            "Train loss : 2.263972 [0/700]\n",
            "Train loss : 2.426476 [64/700]\n",
            "Train loss : 2.281687 [128/700]\n",
            "Train loss : 2.266877 [192/700]\n",
            "Train loss : 2.436285 [256/700]\n",
            "Train loss : 2.203744 [320/700]\n",
            "Train loss : 2.271552 [384/700]\n",
            "Train loss : 2.298504 [448/700]\n",
            "Train loss : 2.187167 [512/700]\n",
            "Train loss : 2.234148 [576/700]\n",
            "Train loss : 2.134275 [600/700]\n",
            "Average Train loss : 2.273153478449041\n",
            "\n",
            "Epoch : 7\n",
            "Train loss : 2.340378 [0/700]\n",
            "Train loss : 2.291335 [64/700]\n",
            "Train loss : 2.440467 [128/700]\n",
            "Train loss : 2.121783 [192/700]\n",
            "Train loss : 2.143875 [256/700]\n",
            "Train loss : 2.251783 [320/700]\n",
            "Train loss : 2.405880 [384/700]\n",
            "Train loss : 2.170034 [448/700]\n",
            "Train loss : 1.973838 [512/700]\n",
            "Train loss : 2.097419 [576/700]\n",
            "Train loss : 2.177625 [600/700]\n",
            "Average Train loss : 2.2194924679669468\n",
            "\n",
            "Epoch : 8\n",
            "Train loss : 2.203194 [0/700]\n",
            "Train loss : 2.032645 [64/700]\n",
            "Train loss : 2.188472 [128/700]\n",
            "Train loss : 2.234695 [192/700]\n",
            "Train loss : 2.207319 [256/700]\n",
            "Train loss : 2.272997 [320/700]\n",
            "Train loss : 1.983411 [384/700]\n",
            "Train loss : 2.180214 [448/700]\n",
            "Train loss : 2.085480 [512/700]\n",
            "Train loss : 2.180827 [576/700]\n",
            "Train loss : 2.282470 [600/700]\n",
            "Average Train loss : 2.1683386022394355\n",
            "\n",
            "Epoch : 9\n",
            "Train loss : 2.116340 [0/700]\n",
            "Train loss : 2.268812 [64/700]\n",
            "Train loss : 2.090439 [128/700]\n",
            "Train loss : 2.186136 [192/700]\n",
            "Train loss : 2.277829 [256/700]\n",
            "Train loss : 2.130367 [320/700]\n",
            "Train loss : 2.117938 [384/700]\n",
            "Train loss : 2.226423 [448/700]\n",
            "Train loss : 2.122300 [512/700]\n",
            "Train loss : 1.812129 [576/700]\n",
            "Train loss : 1.946922 [600/700]\n",
            "Average Train loss : 2.117785096168518\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(sample_X)"
      ],
      "metadata": {
        "id": "l5huX_9lXJ7K"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCWeEUGiXPgc",
        "outputId": "28911a83-3380-4603-976d-218972856626"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.4335, -2.3037,  0.0592,  1.3757, -1.2655, -3.1814,  0.5672,  0.4869,\n",
              "         1.3174, -0.7061], device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTi3uZBXREo",
        "outputId": "e954c3b8-7f0f-4d2e-dd6f-bfbdbd321f75"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyUg9evVXVRd",
        "outputId": "77adddf9-6a34-4b6a-e6b0-b14d57a631f8"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8)"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.argmax(1) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7bcWREPXYso",
        "outputId": "23b0adb7-a85f-462a-c910-534e0cdb27d0"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 1, 8, 7, 8, 8, 7, 8, 8, 8, 8, 4, 1, 8, 8, 1, 1, 7, 7, 8, 8, 8, 1, 8,\n",
              "        8, 1, 1, 8, 7, 8, 8, 8, 8, 8, 8, 1, 1, 8, 1, 7, 8, 1, 1, 1, 8, 8, 7, 7,\n",
              "        1, 8, 8, 1, 8, 1, 7, 8, 8, 8, 7, 8, 8, 1, 1, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.argmax(1) == sample_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMu7Z_lqXiD0",
        "outputId": "ea2806fc-b1b5-4b73-9e3b-e64f3161e39d"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False,  True, False,  True, False,  True,  True, False, False,  True,\n",
              "        False, False,  True, False, False, False, False, False,  True, False,\n",
              "        False, False, False, False,  True, False, False, False,  True,  True,\n",
              "        False,  True,  True, False, False, False, False, False, False,  True,\n",
              "        False,  True,  True, False, False, False,  True,  True, False, False,\n",
              "        False, False,  True, False,  True, False, False, False,  True, False,\n",
              "         True,  True, False, False], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(output.argmax(1) == sample_y).sum().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzEqLoqxX3ww",
        "outputId": "97a50bc7-56c3-4bae-a199-de63acede46a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader, loss_fn,epoch):\n",
        "  total_batch = len(dataloader)\n",
        "  size = len(dataloader.dataset)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  # 경사 계산 X\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for X,y in dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device) \n",
        "\n",
        "      preds = model(X)\n",
        "      loss = loss_fn(preds,y)\n",
        "      correct += (preds.argmax(1) == y).sum().item()\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    test_loss /= total_batch\n",
        "    correct /= size\n",
        "\n",
        "  print(f'Epoch : {epoch} Test Loss : {test_loss}, Test Accuracy : {correct}')\n",
        "\n",
        "  return test_loss\n"
      ],
      "metadata": {
        "id": "qXdbRC9HM7lp"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, valid_loader, criterion,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PH4FQBG0Yf8T",
        "outputId": "26975c6a-85a0-497b-f3b2-e7be171aea77"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 Test Loss : 0.8856472253799439, Test Accuracy : 0.7037037037037037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  train(model,train_loader,optimizer,criterion,epoch)\n",
        "  test(model, valid_loader, criterion,epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i5Iz7RQYsjS",
        "outputId": "eeb30244-96a6-4403-fe40-14e56535ecc5"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "Train loss : 2.129727 [0/700]\n",
            "Train loss : 1.991927 [64/700]\n",
            "Train loss : 2.211837 [128/700]\n",
            "Train loss : 2.163576 [192/700]\n",
            "Train loss : 2.017777 [256/700]\n",
            "Train loss : 2.048154 [320/700]\n",
            "Train loss : 2.122459 [384/700]\n",
            "Train loss : 1.945757 [448/700]\n",
            "Train loss : 2.156959 [512/700]\n",
            "Train loss : 1.934529 [576/700]\n",
            "Train loss : 2.055563 [600/700]\n",
            "Average Train loss : 2.0707514719529585\n",
            "\n",
            "Epoch : 0 Test Loss : 2.0785699129104613, Test Accuracy : 0.22592592592592592\n",
            "Epoch : 1\n",
            "Train loss : 2.076938 [0/700]\n",
            "Train loss : 2.066802 [64/700]\n",
            "Train loss : 2.100272 [128/700]\n",
            "Train loss : 2.145894 [192/700]\n",
            "Train loss : 2.028319 [256/700]\n",
            "Train loss : 2.094425 [320/700]\n",
            "Train loss : 1.876994 [384/700]\n",
            "Train loss : 1.987102 [448/700]\n",
            "Train loss : 1.860853 [512/700]\n",
            "Train loss : 2.147726 [576/700]\n",
            "Train loss : 1.871682 [600/700]\n",
            "Average Train loss : 2.023364381356673\n",
            "\n",
            "Epoch : 1 Test Loss : 2.029289960861206, Test Accuracy : 0.23333333333333334\n",
            "Epoch : 2\n",
            "Train loss : 2.021228 [0/700]\n",
            "Train loss : 2.010167 [64/700]\n",
            "Train loss : 1.883128 [128/700]\n",
            "Train loss : 1.979734 [192/700]\n",
            "Train loss : 1.975275 [256/700]\n",
            "Train loss : 2.055677 [320/700]\n",
            "Train loss : 1.903639 [384/700]\n",
            "Train loss : 2.033051 [448/700]\n",
            "Train loss : 1.968670 [512/700]\n",
            "Train loss : 1.934942 [576/700]\n",
            "Train loss : 2.016158 [600/700]\n",
            "Average Train loss : 1.9801518266851252\n",
            "\n",
            "Epoch : 2 Test Loss : 1.9818861484527588, Test Accuracy : 0.24814814814814815\n",
            "Epoch : 3\n",
            "Train loss : 1.851551 [0/700]\n",
            "Train loss : 1.953097 [64/700]\n",
            "Train loss : 1.883329 [128/700]\n",
            "Train loss : 1.970155 [192/700]\n",
            "Train loss : 1.887724 [256/700]\n",
            "Train loss : 2.087048 [320/700]\n",
            "Train loss : 1.926815 [384/700]\n",
            "Train loss : 1.877336 [448/700]\n",
            "Train loss : 1.858705 [512/700]\n",
            "Train loss : 2.130902 [576/700]\n",
            "Train loss : 1.889360 [600/700]\n",
            "Average Train loss : 1.937820163640109\n",
            "\n",
            "Epoch : 3 Test Loss : 1.935674262046814, Test Accuracy : 0.25555555555555554\n",
            "Epoch : 4\n",
            "Train loss : 1.840995 [0/700]\n",
            "Train loss : 1.827631 [64/700]\n",
            "Train loss : 2.048833 [128/700]\n",
            "Train loss : 2.052162 [192/700]\n",
            "Train loss : 1.819132 [256/700]\n",
            "Train loss : 1.967248 [320/700]\n",
            "Train loss : 1.763749 [384/700]\n",
            "Train loss : 1.834991 [448/700]\n",
            "Train loss : 1.900486 [512/700]\n",
            "Train loss : 2.024649 [576/700]\n",
            "Train loss : 1.783758 [600/700]\n",
            "Average Train loss : 1.896694085814736\n",
            "\n",
            "Epoch : 4 Test Loss : 1.891259455680847, Test Accuracy : 0.25555555555555554\n",
            "Epoch : 5\n",
            "Train loss : 1.717961 [0/700]\n",
            "Train loss : 1.936845 [64/700]\n",
            "Train loss : 1.864543 [128/700]\n",
            "Train loss : 1.787988 [192/700]\n",
            "Train loss : 1.800751 [256/700]\n",
            "Train loss : 1.953391 [320/700]\n",
            "Train loss : 2.070353 [384/700]\n",
            "Train loss : 1.841590 [448/700]\n",
            "Train loss : 1.804266 [512/700]\n",
            "Train loss : 1.811686 [576/700]\n",
            "Train loss : 1.847837 [600/700]\n",
            "Average Train loss : 1.8579280918294734\n",
            "\n",
            "Epoch : 5 Test Loss : 1.8492738962173463, Test Accuracy : 0.25555555555555554\n",
            "Epoch : 6\n",
            "Train loss : 1.885417 [0/700]\n",
            "Train loss : 1.992869 [64/700]\n",
            "Train loss : 1.725272 [128/700]\n",
            "Train loss : 1.866952 [192/700]\n",
            "Train loss : 1.887228 [256/700]\n",
            "Train loss : 1.639828 [320/700]\n",
            "Train loss : 1.937139 [384/700]\n",
            "Train loss : 1.780195 [448/700]\n",
            "Train loss : 1.753683 [512/700]\n",
            "Train loss : 1.853234 [576/700]\n",
            "Train loss : 1.692408 [600/700]\n",
            "Average Train loss : 1.8194749030199917\n",
            "\n",
            "Epoch : 6 Test Loss : 1.8094518899917602, Test Accuracy : 0.26296296296296295\n",
            "Epoch : 7\n",
            "Train loss : 1.776755 [0/700]\n",
            "Train loss : 1.950407 [64/700]\n",
            "Train loss : 1.591353 [128/700]\n",
            "Train loss : 1.741681 [192/700]\n",
            "Train loss : 1.817213 [256/700]\n",
            "Train loss : 1.720103 [320/700]\n",
            "Train loss : 1.807990 [384/700]\n",
            "Train loss : 1.665617 [448/700]\n",
            "Train loss : 1.924195 [512/700]\n",
            "Train loss : 1.714933 [576/700]\n",
            "Train loss : 1.935932 [600/700]\n",
            "Average Train loss : 1.7860163125124844\n",
            "\n",
            "Epoch : 7 Test Loss : 1.7704960584640503, Test Accuracy : 0.25925925925925924\n",
            "Epoch : 8\n",
            "Train loss : 1.801906 [0/700]\n",
            "Train loss : 1.750076 [64/700]\n",
            "Train loss : 1.711496 [128/700]\n",
            "Train loss : 1.971089 [192/700]\n",
            "Train loss : 1.827692 [256/700]\n",
            "Train loss : 1.692061 [320/700]\n",
            "Train loss : 1.748235 [384/700]\n",
            "Train loss : 1.765040 [448/700]\n",
            "Train loss : 1.749549 [512/700]\n",
            "Train loss : 1.593356 [576/700]\n",
            "Train loss : 1.642143 [600/700]\n",
            "Average Train loss : 1.7502403476021506\n",
            "\n",
            "Epoch : 8 Test Loss : 1.7338255167007446, Test Accuracy : 0.25925925925925924\n",
            "Epoch : 9\n",
            "Train loss : 1.592427 [0/700]\n",
            "Train loss : 1.691021 [64/700]\n",
            "Train loss : 1.698216 [128/700]\n",
            "Train loss : 1.670651 [192/700]\n",
            "Train loss : 1.662524 [256/700]\n",
            "Train loss : 1.772876 [320/700]\n",
            "Train loss : 1.795814 [384/700]\n",
            "Train loss : 1.655041 [448/700]\n",
            "Train loss : 1.841539 [512/700]\n",
            "Train loss : 1.715598 [576/700]\n",
            "Train loss : 1.806895 [600/700]\n",
            "Average Train loss : 1.7184183922680942\n",
            "\n",
            "Epoch : 9 Test Loss : 1.6996192693710328, Test Accuracy : 0.25925925925925924\n",
            "Epoch : 10\n",
            "Train loss : 1.580384 [0/700]\n",
            "Train loss : 1.725157 [64/700]\n",
            "Train loss : 1.646961 [128/700]\n",
            "Train loss : 1.632745 [192/700]\n",
            "Train loss : 1.880495 [256/700]\n",
            "Train loss : 1.613311 [320/700]\n",
            "Train loss : 1.625870 [384/700]\n",
            "Train loss : 1.766108 [448/700]\n",
            "Train loss : 1.902028 [512/700]\n",
            "Train loss : 1.578201 [576/700]\n",
            "Train loss : 1.611839 [600/700]\n",
            "Average Train loss : 1.687554337761619\n",
            "\n",
            "Epoch : 10 Test Loss : 1.6655896425247192, Test Accuracy : 0.26296296296296295\n",
            "Epoch : 11\n",
            "Train loss : 1.694839 [0/700]\n",
            "Train loss : 1.768486 [64/700]\n",
            "Train loss : 1.553412 [128/700]\n",
            "Train loss : 1.670316 [192/700]\n",
            "Train loss : 1.623022 [256/700]\n",
            "Train loss : 1.683196 [320/700]\n",
            "Train loss : 1.866639 [384/700]\n",
            "Train loss : 1.543243 [448/700]\n",
            "Train loss : 1.643722 [512/700]\n",
            "Train loss : 1.597409 [576/700]\n",
            "Train loss : 1.597435 [600/700]\n",
            "Average Train loss : 1.6583382541483098\n",
            "\n",
            "Epoch : 11 Test Loss : 1.6335569620132446, Test Accuracy : 0.27037037037037037\n",
            "Epoch : 12\n",
            "Train loss : 1.703499 [0/700]\n",
            "Train loss : 1.698876 [64/700]\n",
            "Train loss : 1.677898 [128/700]\n",
            "Train loss : 1.515538 [192/700]\n",
            "Train loss : 1.641695 [256/700]\n",
            "Train loss : 1.595674 [320/700]\n",
            "Train loss : 1.554714 [384/700]\n",
            "Train loss : 1.670720 [448/700]\n",
            "Train loss : 1.537905 [512/700]\n",
            "Train loss : 1.729948 [576/700]\n",
            "Train loss : 1.606197 [600/700]\n",
            "Average Train loss : 1.6302422501824119\n",
            "\n",
            "Epoch : 12 Test Loss : 1.6034263134002686, Test Accuracy : 0.2814814814814815\n",
            "Epoch : 13\n",
            "Train loss : 1.623767 [0/700]\n",
            "Train loss : 1.623010 [64/700]\n",
            "Train loss : 1.603122 [128/700]\n",
            "Train loss : 1.705688 [192/700]\n",
            "Train loss : 1.628414 [256/700]\n",
            "Train loss : 1.559722 [320/700]\n",
            "Train loss : 1.648751 [384/700]\n",
            "Train loss : 1.580744 [448/700]\n",
            "Train loss : 1.535114 [512/700]\n",
            "Train loss : 1.586375 [576/700]\n",
            "Train loss : 1.542984 [600/700]\n",
            "Average Train loss : 1.603426456451416\n",
            "\n",
            "Epoch : 13 Test Loss : 1.5752022027969361, Test Accuracy : 0.3037037037037037\n",
            "Epoch : 14\n",
            "Train loss : 1.495784 [0/700]\n",
            "Train loss : 1.547902 [64/700]\n",
            "Train loss : 1.563325 [128/700]\n",
            "Train loss : 1.562070 [192/700]\n",
            "Train loss : 1.648016 [256/700]\n",
            "Train loss : 1.490305 [320/700]\n",
            "Train loss : 1.694140 [384/700]\n",
            "Train loss : 1.507285 [448/700]\n",
            "Train loss : 1.747640 [512/700]\n",
            "Train loss : 1.569202 [576/700]\n",
            "Train loss : 1.535768 [600/700]\n",
            "Average Train loss : 1.5783124945380471\n",
            "\n",
            "Epoch : 14 Test Loss : 1.5487631797790526, Test Accuracy : 0.3148148148148148\n",
            "Epoch : 15\n",
            "Train loss : 1.567177 [0/700]\n",
            "Train loss : 1.586174 [64/700]\n",
            "Train loss : 1.566195 [128/700]\n",
            "Train loss : 1.545832 [192/700]\n",
            "Train loss : 1.637159 [256/700]\n",
            "Train loss : 1.635509 [320/700]\n",
            "Train loss : 1.496631 [384/700]\n",
            "Train loss : 1.428749 [448/700]\n",
            "Train loss : 1.560063 [512/700]\n",
            "Train loss : 1.534536 [576/700]\n",
            "Train loss : 1.546655 [600/700]\n",
            "Average Train loss : 1.5549709580161355\n",
            "\n",
            "Epoch : 15 Test Loss : 1.5234807491302491, Test Accuracy : 0.3333333333333333\n",
            "Epoch : 16\n",
            "Train loss : 1.592234 [0/700]\n",
            "Train loss : 1.522867 [64/700]\n",
            "Train loss : 1.633043 [128/700]\n",
            "Train loss : 1.503678 [192/700]\n",
            "Train loss : 1.563106 [256/700]\n",
            "Train loss : 1.542912 [320/700]\n",
            "Train loss : 1.524978 [384/700]\n",
            "Train loss : 1.427016 [448/700]\n",
            "Train loss : 1.532852 [512/700]\n",
            "Train loss : 1.481170 [576/700]\n",
            "Train loss : 1.534148 [600/700]\n",
            "Average Train loss : 1.532545956698331\n",
            "\n",
            "Epoch : 16 Test Loss : 1.4990271091461183, Test Accuracy : 0.3296296296296296\n",
            "Epoch : 17\n",
            "Train loss : 1.399738 [0/700]\n",
            "Train loss : 1.505566 [64/700]\n",
            "Train loss : 1.454154 [128/700]\n",
            "Train loss : 1.467252 [192/700]\n",
            "Train loss : 1.563077 [256/700]\n",
            "Train loss : 1.581227 [320/700]\n",
            "Train loss : 1.451467 [384/700]\n",
            "Train loss : 1.608314 [448/700]\n",
            "Train loss : 1.515091 [512/700]\n",
            "Train loss : 1.459057 [576/700]\n",
            "Train loss : 1.625258 [600/700]\n",
            "Average Train loss : 1.5118364962664517\n",
            "\n",
            "Epoch : 17 Test Loss : 1.475732183456421, Test Accuracy : 0.34444444444444444\n",
            "Epoch : 18\n",
            "Train loss : 1.504667 [0/700]\n",
            "Train loss : 1.596957 [64/700]\n",
            "Train loss : 1.392350 [128/700]\n",
            "Train loss : 1.488718 [192/700]\n",
            "Train loss : 1.539596 [256/700]\n",
            "Train loss : 1.375704 [320/700]\n",
            "Train loss : 1.531445 [384/700]\n",
            "Train loss : 1.395320 [448/700]\n",
            "Train loss : 1.531142 [512/700]\n",
            "Train loss : 1.537869 [576/700]\n",
            "Train loss : 1.504470 [600/700]\n",
            "Average Train loss : 1.4907489039681174\n",
            "\n",
            "Epoch : 18 Test Loss : 1.4541317701339722, Test Accuracy : 0.3592592592592593\n",
            "Epoch : 19\n",
            "Train loss : 1.450449 [0/700]\n",
            "Train loss : 1.388012 [64/700]\n",
            "Train loss : 1.522018 [128/700]\n",
            "Train loss : 1.485048 [192/700]\n",
            "Train loss : 1.530351 [256/700]\n",
            "Train loss : 1.306679 [320/700]\n",
            "Train loss : 1.417061 [384/700]\n",
            "Train loss : 1.589406 [448/700]\n",
            "Train loss : 1.503259 [512/700]\n",
            "Train loss : 1.533953 [576/700]\n",
            "Train loss : 1.457573 [600/700]\n",
            "Average Train loss : 1.4712552265687422\n",
            "\n",
            "Epoch : 19 Test Loss : 1.4338297367095947, Test Accuracy : 0.4074074074074074\n",
            "Epoch : 20\n",
            "Train loss : 1.356956 [0/700]\n",
            "Train loss : 1.597690 [64/700]\n",
            "Train loss : 1.441817 [128/700]\n",
            "Train loss : 1.534783 [192/700]\n",
            "Train loss : 1.440847 [256/700]\n",
            "Train loss : 1.288117 [320/700]\n",
            "Train loss : 1.628937 [384/700]\n",
            "Train loss : 1.478659 [448/700]\n",
            "Train loss : 1.402155 [512/700]\n",
            "Train loss : 1.244161 [576/700]\n",
            "Train loss : 1.577748 [600/700]\n",
            "Average Train loss : 1.4538063569502397\n",
            "\n",
            "Epoch : 20 Test Loss : 1.4140026092529296, Test Accuracy : 0.4444444444444444\n",
            "Epoch : 21\n",
            "Train loss : 1.521536 [0/700]\n",
            "Train loss : 1.505583 [64/700]\n",
            "Train loss : 1.523567 [128/700]\n",
            "Train loss : 1.365814 [192/700]\n",
            "Train loss : 1.304698 [256/700]\n",
            "Train loss : 1.516622 [320/700]\n",
            "Train loss : 1.631635 [384/700]\n",
            "Train loss : 1.358614 [448/700]\n",
            "Train loss : 1.349187 [512/700]\n",
            "Train loss : 1.338372 [576/700]\n",
            "Train loss : 1.369643 [600/700]\n",
            "Average Train loss : 1.4350245649164373\n",
            "\n",
            "Epoch : 21 Test Loss : 1.3956552267074585, Test Accuracy : 0.43703703703703706\n",
            "Epoch : 22\n",
            "Train loss : 1.250376 [0/700]\n",
            "Train loss : 1.585423 [64/700]\n",
            "Train loss : 1.497325 [128/700]\n",
            "Train loss : 1.531732 [192/700]\n",
            "Train loss : 1.307474 [256/700]\n",
            "Train loss : 1.333803 [320/700]\n",
            "Train loss : 1.467406 [384/700]\n",
            "Train loss : 1.321499 [448/700]\n",
            "Train loss : 1.480366 [512/700]\n",
            "Train loss : 1.381776 [576/700]\n",
            "Train loss : 1.451853 [600/700]\n",
            "Average Train loss : 1.4190029556101018\n",
            "\n",
            "Epoch : 22 Test Loss : 1.3781058311462402, Test Accuracy : 0.44814814814814813\n",
            "Epoch : 23\n",
            "Train loss : 1.360254 [0/700]\n",
            "Train loss : 1.414908 [64/700]\n",
            "Train loss : 1.388448 [128/700]\n",
            "Train loss : 1.441316 [192/700]\n",
            "Train loss : 1.434516 [256/700]\n",
            "Train loss : 1.448341 [320/700]\n",
            "Train loss : 1.515684 [384/700]\n",
            "Train loss : 1.378994 [448/700]\n",
            "Train loss : 1.445476 [512/700]\n",
            "Train loss : 1.292767 [576/700]\n",
            "Train loss : 1.304288 [600/700]\n",
            "Average Train loss : 1.4022719751704822\n",
            "\n",
            "Epoch : 23 Test Loss : 1.361430859565735, Test Accuracy : 0.46296296296296297\n",
            "Epoch : 24\n",
            "Train loss : 1.445113 [0/700]\n",
            "Train loss : 1.430336 [64/700]\n",
            "Train loss : 1.343336 [128/700]\n",
            "Train loss : 1.278411 [192/700]\n",
            "Train loss : 1.327917 [256/700]\n",
            "Train loss : 1.254188 [320/700]\n",
            "Train loss : 1.382032 [384/700]\n",
            "Train loss : 1.341075 [448/700]\n",
            "Train loss : 1.552498 [512/700]\n",
            "Train loss : 1.512094 [576/700]\n",
            "Train loss : 1.399384 [600/700]\n",
            "Average Train loss : 1.3878530697389082\n",
            "\n",
            "Epoch : 24 Test Loss : 1.3455333948135375, Test Accuracy : 0.45925925925925926\n",
            "Epoch : 25\n",
            "Train loss : 1.450880 [0/700]\n",
            "Train loss : 1.496419 [64/700]\n",
            "Train loss : 1.228401 [128/700]\n",
            "Train loss : 1.316457 [192/700]\n",
            "Train loss : 1.391513 [256/700]\n",
            "Train loss : 1.168686 [320/700]\n",
            "Train loss : 1.455809 [384/700]\n",
            "Train loss : 1.487477 [448/700]\n",
            "Train loss : 1.334531 [512/700]\n",
            "Train loss : 1.414944 [576/700]\n",
            "Train loss : 1.358738 [600/700]\n",
            "Average Train loss : 1.3730776960199529\n",
            "\n",
            "Epoch : 25 Test Loss : 1.3305413246154785, Test Accuracy : 0.45925925925925926\n",
            "Epoch : 26\n",
            "Train loss : 1.350908 [0/700]\n",
            "Train loss : 1.308419 [64/700]\n",
            "Train loss : 1.408805 [128/700]\n",
            "Train loss : 1.405546 [192/700]\n",
            "Train loss : 1.333339 [256/700]\n",
            "Train loss : 1.405831 [320/700]\n",
            "Train loss : 1.265713 [384/700]\n",
            "Train loss : 1.447824 [448/700]\n",
            "Train loss : 1.402667 [512/700]\n",
            "Train loss : 1.262629 [576/700]\n",
            "Train loss : 1.361061 [600/700]\n",
            "Average Train loss : 1.3593402992595325\n",
            "\n",
            "Epoch : 26 Test Loss : 1.315822958946228, Test Accuracy : 0.46296296296296297\n",
            "Epoch : 27\n",
            "Train loss : 1.350985 [0/700]\n",
            "Train loss : 1.403641 [64/700]\n",
            "Train loss : 1.342111 [128/700]\n",
            "Train loss : 1.342553 [192/700]\n",
            "Train loss : 1.349135 [256/700]\n",
            "Train loss : 1.316769 [320/700]\n",
            "Train loss : 1.301406 [384/700]\n",
            "Train loss : 1.338426 [448/700]\n",
            "Train loss : 1.393327 [512/700]\n",
            "Train loss : 1.384366 [576/700]\n",
            "Train loss : 1.277719 [600/700]\n",
            "Average Train loss : 1.345494346185164\n",
            "\n",
            "Epoch : 27 Test Loss : 1.3021239995956422, Test Accuracy : 0.48148148148148145\n",
            "Epoch : 28\n",
            "Train loss : 1.321369 [0/700]\n",
            "Train loss : 1.332097 [64/700]\n",
            "Train loss : 1.049342 [128/700]\n",
            "Train loss : 1.456079 [192/700]\n",
            "Train loss : 1.203390 [256/700]\n",
            "Train loss : 1.410324 [320/700]\n",
            "Train loss : 1.365841 [384/700]\n",
            "Train loss : 1.423100 [448/700]\n",
            "Train loss : 1.382041 [512/700]\n",
            "Train loss : 1.276732 [576/700]\n",
            "Train loss : 1.452711 [600/700]\n",
            "Average Train loss : 1.3339114514264194\n",
            "\n",
            "Epoch : 28 Test Loss : 1.2888416290283202, Test Accuracy : 0.48518518518518516\n",
            "Epoch : 29\n",
            "Train loss : 1.406057 [0/700]\n",
            "Train loss : 1.419778 [64/700]\n",
            "Train loss : 1.313865 [128/700]\n",
            "Train loss : 1.329894 [192/700]\n",
            "Train loss : 1.264725 [256/700]\n",
            "Train loss : 1.262652 [320/700]\n",
            "Train loss : 1.158094 [384/700]\n",
            "Train loss : 1.417873 [448/700]\n",
            "Train loss : 1.278615 [512/700]\n",
            "Train loss : 1.417856 [576/700]\n",
            "Train loss : 1.258903 [600/700]\n",
            "Average Train loss : 1.3207556334408848\n",
            "\n",
            "Epoch : 29 Test Loss : 1.2758983612060546, Test Accuracy : 0.48518518518518516\n",
            "Epoch : 30\n",
            "Train loss : 1.270338 [0/700]\n",
            "Train loss : 1.348839 [64/700]\n",
            "Train loss : 1.264871 [128/700]\n",
            "Train loss : 1.364923 [192/700]\n",
            "Train loss : 1.277014 [256/700]\n",
            "Train loss : 1.418553 [320/700]\n",
            "Train loss : 1.432119 [384/700]\n",
            "Train loss : 1.280109 [448/700]\n",
            "Train loss : 1.274533 [512/700]\n",
            "Train loss : 1.168746 [576/700]\n",
            "Train loss : 1.302018 [600/700]\n",
            "Average Train loss : 1.309278498996388\n",
            "\n",
            "Epoch : 30 Test Loss : 1.263562846183777, Test Accuracy : 0.4888888888888889\n",
            "Epoch : 31\n",
            "Train loss : 1.246491 [0/700]\n",
            "Train loss : 1.301379 [64/700]\n",
            "Train loss : 1.220986 [128/700]\n",
            "Train loss : 1.361763 [192/700]\n",
            "Train loss : 1.269430 [256/700]\n",
            "Train loss : 1.366608 [320/700]\n",
            "Train loss : 1.225116 [384/700]\n",
            "Train loss : 1.391595 [448/700]\n",
            "Train loss : 1.355176 [512/700]\n",
            "Train loss : 1.336365 [576/700]\n",
            "Train loss : 1.194459 [600/700]\n",
            "Average Train loss : 1.2972152883356267\n",
            "\n",
            "Epoch : 31 Test Loss : 1.2520729064941407, Test Accuracy : 0.4888888888888889\n",
            "Epoch : 32\n",
            "Train loss : 1.316466 [0/700]\n",
            "Train loss : 1.315603 [64/700]\n",
            "Train loss : 1.265576 [128/700]\n",
            "Train loss : 1.189734 [192/700]\n",
            "Train loss : 1.288395 [256/700]\n",
            "Train loss : 1.317204 [320/700]\n",
            "Train loss : 1.340095 [384/700]\n",
            "Train loss : 1.243214 [448/700]\n",
            "Train loss : 1.361076 [512/700]\n",
            "Train loss : 1.310512 [576/700]\n",
            "Train loss : 1.205166 [600/700]\n",
            "Average Train loss : 1.2866401347247036\n",
            "\n",
            "Epoch : 32 Test Loss : 1.2408637762069703, Test Accuracy : 0.5\n",
            "Epoch : 33\n",
            "Train loss : 1.290998 [0/700]\n",
            "Train loss : 1.207319 [64/700]\n",
            "Train loss : 1.331153 [128/700]\n",
            "Train loss : 1.431829 [192/700]\n",
            "Train loss : 1.274182 [256/700]\n",
            "Train loss : 1.375725 [320/700]\n",
            "Train loss : 1.220992 [384/700]\n",
            "Train loss : 1.161079 [448/700]\n",
            "Train loss : 1.314709 [512/700]\n",
            "Train loss : 1.120510 [576/700]\n",
            "Train loss : 1.315985 [600/700]\n",
            "Average Train loss : 1.276770981875333\n",
            "\n",
            "Epoch : 33 Test Loss : 1.229936122894287, Test Accuracy : 0.5037037037037037\n",
            "Epoch : 34\n",
            "Train loss : 1.366298 [0/700]\n",
            "Train loss : 1.323811 [64/700]\n",
            "Train loss : 1.420551 [128/700]\n",
            "Train loss : 1.198887 [192/700]\n",
            "Train loss : 1.126608 [256/700]\n",
            "Train loss : 1.282503 [320/700]\n",
            "Train loss : 1.238777 [384/700]\n",
            "Train loss : 1.254669 [448/700]\n",
            "Train loss : 1.412254 [512/700]\n",
            "Train loss : 1.182369 [576/700]\n",
            "Train loss : 1.114466 [600/700]\n",
            "Average Train loss : 1.2655630653554744\n",
            "\n",
            "Epoch : 34 Test Loss : 1.219651985168457, Test Accuracy : 0.5148148148148148\n",
            "Epoch : 35\n",
            "Train loss : 1.347835 [0/700]\n",
            "Train loss : 1.196274 [64/700]\n",
            "Train loss : 1.314522 [128/700]\n",
            "Train loss : 1.200318 [192/700]\n",
            "Train loss : 1.219073 [256/700]\n",
            "Train loss : 1.327476 [320/700]\n",
            "Train loss : 1.144628 [384/700]\n",
            "Train loss : 1.339963 [448/700]\n",
            "Train loss : 1.304677 [512/700]\n",
            "Train loss : 1.176750 [576/700]\n",
            "Train loss : 1.252909 [600/700]\n",
            "Average Train loss : 1.2567658641121604\n",
            "\n",
            "Epoch : 35 Test Loss : 1.2091020345687866, Test Accuracy : 0.5370370370370371\n",
            "Epoch : 36\n",
            "Train loss : 1.360091 [0/700]\n",
            "Train loss : 1.233525 [64/700]\n",
            "Train loss : 1.302236 [128/700]\n",
            "Train loss : 1.274993 [192/700]\n",
            "Train loss : 1.237915 [256/700]\n",
            "Train loss : 1.067450 [320/700]\n",
            "Train loss : 1.288154 [384/700]\n",
            "Train loss : 1.242357 [448/700]\n",
            "Train loss : 1.253476 [512/700]\n",
            "Train loss : 1.204733 [576/700]\n",
            "Train loss : 1.253836 [600/700]\n",
            "Average Train loss : 1.2471604563973167\n",
            "\n",
            "Epoch : 36 Test Loss : 1.1997499227523805, Test Accuracy : 0.562962962962963\n",
            "Epoch : 37\n",
            "Train loss : 1.116068 [0/700]\n",
            "Train loss : 1.402459 [64/700]\n",
            "Train loss : 1.020744 [128/700]\n",
            "Train loss : 1.347334 [192/700]\n",
            "Train loss : 1.360139 [256/700]\n",
            "Train loss : 1.120685 [320/700]\n",
            "Train loss : 1.276234 [384/700]\n",
            "Train loss : 1.317570 [448/700]\n",
            "Train loss : 1.192747 [512/700]\n",
            "Train loss : 1.169000 [576/700]\n",
            "Train loss : 1.299898 [600/700]\n",
            "Average Train loss : 1.2384434830058704\n",
            "\n",
            "Epoch : 37 Test Loss : 1.1904094457626342, Test Accuracy : 0.5777777777777777\n",
            "Epoch : 38\n",
            "Train loss : 1.172357 [0/700]\n",
            "Train loss : 1.296483 [64/700]\n",
            "Train loss : 1.236437 [128/700]\n",
            "Train loss : 1.220269 [192/700]\n",
            "Train loss : 1.348477 [256/700]\n",
            "Train loss : 1.230420 [320/700]\n",
            "Train loss : 1.203434 [384/700]\n",
            "Train loss : 1.231259 [448/700]\n",
            "Train loss : 1.242192 [512/700]\n",
            "Train loss : 1.203164 [576/700]\n",
            "Train loss : 1.130558 [600/700]\n",
            "Average Train loss : 1.2286407622424038\n",
            "\n",
            "Epoch : 38 Test Loss : 1.1814342260360717, Test Accuracy : 0.5851851851851851\n",
            "Epoch : 39\n",
            "Train loss : 1.150020 [0/700]\n",
            "Train loss : 1.221186 [64/700]\n",
            "Train loss : 1.270147 [128/700]\n",
            "Train loss : 1.215734 [192/700]\n",
            "Train loss : 1.257916 [256/700]\n",
            "Train loss : 1.177063 [320/700]\n",
            "Train loss : 1.218602 [384/700]\n",
            "Train loss : 1.213885 [448/700]\n",
            "Train loss : 1.221250 [512/700]\n",
            "Train loss : 1.402656 [576/700]\n",
            "Train loss : 1.068720 [600/700]\n",
            "Average Train loss : 1.2197435985911975\n",
            "\n",
            "Epoch : 39 Test Loss : 1.1727552175521851, Test Accuracy : 0.6\n",
            "Epoch : 40\n",
            "Train loss : 1.216336 [0/700]\n",
            "Train loss : 1.214382 [64/700]\n",
            "Train loss : 1.177831 [128/700]\n",
            "Train loss : 1.221480 [192/700]\n",
            "Train loss : 1.223836 [256/700]\n",
            "Train loss : 1.207986 [320/700]\n",
            "Train loss : 1.198118 [384/700]\n",
            "Train loss : 1.299117 [448/700]\n",
            "Train loss : 1.235044 [512/700]\n",
            "Train loss : 1.178675 [576/700]\n",
            "Train loss : 1.160416 [600/700]\n",
            "Average Train loss : 1.2121110395951704\n",
            "\n",
            "Epoch : 40 Test Loss : 1.16407949924469, Test Accuracy : 0.6185185185185185\n",
            "Epoch : 41\n",
            "Train loss : 1.083169 [0/700]\n",
            "Train loss : 1.426553 [64/700]\n",
            "Train loss : 1.360721 [128/700]\n",
            "Train loss : 1.153291 [192/700]\n",
            "Train loss : 1.241627 [256/700]\n",
            "Train loss : 1.107540 [320/700]\n",
            "Train loss : 1.214378 [384/700]\n",
            "Train loss : 1.285538 [448/700]\n",
            "Train loss : 1.143733 [512/700]\n",
            "Train loss : 1.223867 [576/700]\n",
            "Train loss : 0.995146 [600/700]\n",
            "Average Train loss : 1.2032331174070185\n",
            "\n",
            "Epoch : 41 Test Loss : 1.156019115447998, Test Accuracy : 0.6148148148148148\n",
            "Epoch : 42\n",
            "Train loss : 1.231863 [0/700]\n",
            "Train loss : 1.189132 [64/700]\n",
            "Train loss : 1.298003 [128/700]\n",
            "Train loss : 0.955961 [192/700]\n",
            "Train loss : 1.210039 [256/700]\n",
            "Train loss : 1.266438 [320/700]\n",
            "Train loss : 1.199877 [384/700]\n",
            "Train loss : 1.160984 [448/700]\n",
            "Train loss : 1.184176 [512/700]\n",
            "Train loss : 1.289740 [576/700]\n",
            "Train loss : 1.174180 [600/700]\n",
            "Average Train loss : 1.1963992335579612\n",
            "\n",
            "Epoch : 42 Test Loss : 1.1479507446289063, Test Accuracy : 0.6222222222222222\n",
            "Epoch : 43\n",
            "Train loss : 1.180468 [0/700]\n",
            "Train loss : 1.232171 [64/700]\n",
            "Train loss : 1.344109 [128/700]\n",
            "Train loss : 1.254100 [192/700]\n",
            "Train loss : 1.148769 [256/700]\n",
            "Train loss : 1.174411 [320/700]\n",
            "Train loss : 1.195707 [384/700]\n",
            "Train loss : 1.171234 [448/700]\n",
            "Train loss : 1.115538 [512/700]\n",
            "Train loss : 1.097738 [576/700]\n",
            "Train loss : 1.161405 [600/700]\n",
            "Average Train loss : 1.1886954307556152\n",
            "\n",
            "Epoch : 43 Test Loss : 1.1402339458465576, Test Accuracy : 0.6222222222222222\n",
            "Epoch : 44\n",
            "Train loss : 1.128982 [0/700]\n",
            "Train loss : 1.225710 [64/700]\n",
            "Train loss : 1.138712 [128/700]\n",
            "Train loss : 0.997227 [192/700]\n",
            "Train loss : 1.171666 [256/700]\n",
            "Train loss : 1.130530 [320/700]\n",
            "Train loss : 1.186806 [384/700]\n",
            "Train loss : 1.316749 [448/700]\n",
            "Train loss : 1.303162 [512/700]\n",
            "Train loss : 1.147423 [576/700]\n",
            "Train loss : 1.254683 [600/700]\n",
            "Average Train loss : 1.181968325918371\n",
            "\n",
            "Epoch : 44 Test Loss : 1.1326949834823608, Test Accuracy : 0.6370370370370371\n",
            "Epoch : 45\n",
            "Train loss : 1.205638 [0/700]\n",
            "Train loss : 1.154972 [64/700]\n",
            "Train loss : 1.155745 [128/700]\n",
            "Train loss : 1.109915 [192/700]\n",
            "Train loss : 1.211852 [256/700]\n",
            "Train loss : 1.344776 [320/700]\n",
            "Train loss : 1.232189 [384/700]\n",
            "Train loss : 1.083606 [448/700]\n",
            "Train loss : 1.014319 [512/700]\n",
            "Train loss : 1.220384 [576/700]\n",
            "Train loss : 1.184796 [600/700]\n",
            "Average Train loss : 1.174380974336104\n",
            "\n",
            "Epoch : 45 Test Loss : 1.1253203868865966, Test Accuracy : 0.6407407407407407\n",
            "Epoch : 46\n",
            "Train loss : 1.262681 [0/700]\n",
            "Train loss : 1.163831 [64/700]\n",
            "Train loss : 1.195446 [128/700]\n",
            "Train loss : 1.089494 [192/700]\n",
            "Train loss : 1.165911 [256/700]\n",
            "Train loss : 1.075634 [320/700]\n",
            "Train loss : 1.210008 [384/700]\n",
            "Train loss : 1.246760 [448/700]\n",
            "Train loss : 1.180189 [512/700]\n",
            "Train loss : 1.222331 [576/700]\n",
            "Train loss : 1.019131 [600/700]\n",
            "Average Train loss : 1.1664924296465786\n",
            "\n",
            "Epoch : 46 Test Loss : 1.1183450102806092, Test Accuracy : 0.6481481481481481\n",
            "Epoch : 47\n",
            "Train loss : 1.276321 [0/700]\n",
            "Train loss : 1.034164 [64/700]\n",
            "Train loss : 1.151069 [128/700]\n",
            "Train loss : 1.211956 [192/700]\n",
            "Train loss : 1.107825 [256/700]\n",
            "Train loss : 1.154342 [320/700]\n",
            "Train loss : 1.211580 [384/700]\n",
            "Train loss : 1.111836 [448/700]\n",
            "Train loss : 1.298398 [512/700]\n",
            "Train loss : 1.217623 [576/700]\n",
            "Train loss : 0.979708 [600/700]\n",
            "Average Train loss : 1.1595292416485874\n",
            "\n",
            "Epoch : 47 Test Loss : 1.1112722158432007, Test Accuracy : 0.6555555555555556\n",
            "Epoch : 48\n",
            "Train loss : 1.144054 [0/700]\n",
            "Train loss : 1.026874 [64/700]\n",
            "Train loss : 1.094480 [128/700]\n",
            "Train loss : 1.094728 [192/700]\n",
            "Train loss : 1.216449 [256/700]\n",
            "Train loss : 1.168482 [320/700]\n",
            "Train loss : 1.286862 [384/700]\n",
            "Train loss : 1.205833 [448/700]\n",
            "Train loss : 1.029752 [512/700]\n",
            "Train loss : 1.222515 [576/700]\n",
            "Train loss : 1.205912 [600/700]\n",
            "Average Train loss : 1.1541765169663862\n",
            "\n",
            "Epoch : 48 Test Loss : 1.104630959033966, Test Accuracy : 0.6555555555555556\n",
            "Epoch : 49\n",
            "Train loss : 1.152886 [0/700]\n",
            "Train loss : 1.141544 [64/700]\n",
            "Train loss : 1.169068 [128/700]\n",
            "Train loss : 1.084991 [192/700]\n",
            "Train loss : 1.030794 [256/700]\n",
            "Train loss : 1.217563 [320/700]\n",
            "Train loss : 1.057947 [384/700]\n",
            "Train loss : 1.217541 [448/700]\n",
            "Train loss : 1.173424 [512/700]\n",
            "Train loss : 1.089185 [576/700]\n",
            "Train loss : 1.295588 [600/700]\n",
            "Average Train loss : 1.1482301625338467\n",
            "\n",
            "Epoch : 49 Test Loss : 1.098182785511017, Test Accuracy : 0.6555555555555556\n",
            "Epoch : 50\n",
            "Train loss : 1.045120 [0/700]\n",
            "Train loss : 1.137171 [64/700]\n",
            "Train loss : 1.279079 [128/700]\n",
            "Train loss : 1.010701 [192/700]\n",
            "Train loss : 1.170941 [256/700]\n",
            "Train loss : 1.111044 [320/700]\n",
            "Train loss : 1.083549 [384/700]\n",
            "Train loss : 1.112440 [448/700]\n",
            "Train loss : 1.219394 [512/700]\n",
            "Train loss : 1.098021 [576/700]\n",
            "Train loss : 1.294153 [600/700]\n",
            "Average Train loss : 1.1419649124145508\n",
            "\n",
            "Epoch : 50 Test Loss : 1.0915187120437622, Test Accuracy : 0.6518518518518519\n",
            "Epoch : 51\n",
            "Train loss : 1.080636 [0/700]\n",
            "Train loss : 1.099497 [64/700]\n",
            "Train loss : 1.062747 [128/700]\n",
            "Train loss : 1.198170 [192/700]\n",
            "Train loss : 1.187387 [256/700]\n",
            "Train loss : 1.193923 [320/700]\n",
            "Train loss : 1.157871 [384/700]\n",
            "Train loss : 1.259533 [448/700]\n",
            "Train loss : 1.141704 [512/700]\n",
            "Train loss : 1.145397 [576/700]\n",
            "Train loss : 0.946879 [600/700]\n",
            "Average Train loss : 1.133976768363606\n",
            "\n",
            "Epoch : 51 Test Loss : 1.085489320755005, Test Accuracy : 0.6518518518518519\n",
            "Epoch : 52\n",
            "Train loss : 1.207915 [0/700]\n",
            "Train loss : 1.024105 [64/700]\n",
            "Train loss : 1.234948 [128/700]\n",
            "Train loss : 1.152301 [192/700]\n",
            "Train loss : 1.161104 [256/700]\n",
            "Train loss : 1.193609 [320/700]\n",
            "Train loss : 1.134125 [384/700]\n",
            "Train loss : 1.109229 [448/700]\n",
            "Train loss : 1.018894 [512/700]\n",
            "Train loss : 1.112557 [576/700]\n",
            "Train loss : 1.065253 [600/700]\n",
            "Average Train loss : 1.1285491423173384\n",
            "\n",
            "Epoch : 52 Test Loss : 1.079568576812744, Test Accuracy : 0.6555555555555556\n",
            "Epoch : 53\n",
            "Train loss : 1.121501 [0/700]\n",
            "Train loss : 1.082271 [64/700]\n",
            "Train loss : 1.180917 [128/700]\n",
            "Train loss : 1.056594 [192/700]\n",
            "Train loss : 1.126086 [256/700]\n",
            "Train loss : 1.102622 [320/700]\n",
            "Train loss : 1.184450 [384/700]\n",
            "Train loss : 1.122774 [448/700]\n",
            "Train loss : 1.183382 [512/700]\n",
            "Train loss : 1.181017 [576/700]\n",
            "Train loss : 1.003512 [600/700]\n",
            "Average Train loss : 1.1222841306166216\n",
            "\n",
            "Epoch : 53 Test Loss : 1.0735032081604003, Test Accuracy : 0.6555555555555556\n",
            "Epoch : 54\n",
            "Train loss : 1.275836 [0/700]\n",
            "Train loss : 1.180384 [64/700]\n",
            "Train loss : 0.949617 [128/700]\n",
            "Train loss : 1.169004 [192/700]\n",
            "Train loss : 0.911154 [256/700]\n",
            "Train loss : 1.018923 [320/700]\n",
            "Train loss : 1.123229 [384/700]\n",
            "Train loss : 1.355978 [448/700]\n",
            "Train loss : 1.123604 [512/700]\n",
            "Train loss : 1.002185 [576/700]\n",
            "Train loss : 1.183447 [600/700]\n",
            "Average Train loss : 1.117578316818584\n",
            "\n",
            "Epoch : 54 Test Loss : 1.0674675226211547, Test Accuracy : 0.6592592592592592\n",
            "Epoch : 55\n",
            "Train loss : 1.154724 [0/700]\n",
            "Train loss : 0.915741 [64/700]\n",
            "Train loss : 1.169499 [128/700]\n",
            "Train loss : 1.068401 [192/700]\n",
            "Train loss : 1.052619 [256/700]\n",
            "Train loss : 1.168969 [320/700]\n",
            "Train loss : 1.168158 [384/700]\n",
            "Train loss : 1.093820 [448/700]\n",
            "Train loss : 1.186061 [512/700]\n",
            "Train loss : 1.125352 [576/700]\n",
            "Train loss : 1.124524 [600/700]\n",
            "Average Train loss : 1.1116244088519702\n",
            "\n",
            "Epoch : 55 Test Loss : 1.0618918895721436, Test Accuracy : 0.662962962962963\n",
            "Epoch : 56\n",
            "Train loss : 1.198750 [0/700]\n",
            "Train loss : 1.099062 [64/700]\n",
            "Train loss : 1.123750 [128/700]\n",
            "Train loss : 1.076693 [192/700]\n",
            "Train loss : 1.200976 [256/700]\n",
            "Train loss : 1.090099 [320/700]\n",
            "Train loss : 0.911818 [384/700]\n",
            "Train loss : 1.007160 [448/700]\n",
            "Train loss : 1.171453 [512/700]\n",
            "Train loss : 1.123433 [576/700]\n",
            "Train loss : 1.166983 [600/700]\n",
            "Average Train loss : 1.1063797961581836\n",
            "\n",
            "Epoch : 56 Test Loss : 1.0564282894134522, Test Accuracy : 0.6666666666666666\n",
            "Epoch : 57\n",
            "Train loss : 1.251361 [0/700]\n",
            "Train loss : 1.084130 [64/700]\n",
            "Train loss : 1.030051 [128/700]\n",
            "Train loss : 1.082396 [192/700]\n",
            "Train loss : 1.135031 [256/700]\n",
            "Train loss : 1.187747 [320/700]\n",
            "Train loss : 1.090714 [384/700]\n",
            "Train loss : 1.068537 [448/700]\n",
            "Train loss : 1.037089 [512/700]\n",
            "Train loss : 1.038521 [576/700]\n",
            "Train loss : 1.101102 [600/700]\n",
            "Average Train loss : 1.1006071459163318\n",
            "\n",
            "Epoch : 57 Test Loss : 1.0507636547088623, Test Accuracy : 0.674074074074074\n",
            "Epoch : 58\n",
            "Train loss : 1.005048 [0/700]\n",
            "Train loss : 1.066885 [64/700]\n",
            "Train loss : 1.094746 [128/700]\n",
            "Train loss : 1.133418 [192/700]\n",
            "Train loss : 1.100112 [256/700]\n",
            "Train loss : 1.111418 [320/700]\n",
            "Train loss : 1.141376 [384/700]\n",
            "Train loss : 1.248997 [448/700]\n",
            "Train loss : 1.101546 [512/700]\n",
            "Train loss : 0.923890 [576/700]\n",
            "Train loss : 1.122503 [600/700]\n",
            "Average Train loss : 1.0954491279341958\n",
            "\n",
            "Epoch : 58 Test Loss : 1.0456428050994873, Test Accuracy : 0.6703703703703704\n",
            "Epoch : 59\n",
            "Train loss : 1.171336 [0/700]\n",
            "Train loss : 1.113368 [64/700]\n",
            "Train loss : 0.955115 [128/700]\n",
            "Train loss : 1.077734 [192/700]\n",
            "Train loss : 1.169351 [256/700]\n",
            "Train loss : 1.104135 [320/700]\n",
            "Train loss : 1.105918 [384/700]\n",
            "Train loss : 1.133180 [448/700]\n",
            "Train loss : 1.058109 [512/700]\n",
            "Train loss : 1.124564 [576/700]\n",
            "Train loss : 0.970492 [600/700]\n",
            "Average Train loss : 1.0893910310485146\n",
            "\n",
            "Epoch : 59 Test Loss : 1.040409469604492, Test Accuracy : 0.6703703703703704\n",
            "Epoch : 60\n",
            "Train loss : 1.091623 [0/700]\n",
            "Train loss : 1.045270 [64/700]\n",
            "Train loss : 1.120787 [128/700]\n",
            "Train loss : 1.052870 [192/700]\n",
            "Train loss : 1.026805 [256/700]\n",
            "Train loss : 0.948040 [320/700]\n",
            "Train loss : 1.092987 [384/700]\n",
            "Train loss : 1.257999 [448/700]\n",
            "Train loss : 1.111915 [512/700]\n",
            "Train loss : 1.126075 [576/700]\n",
            "Train loss : 1.058685 [600/700]\n",
            "Average Train loss : 1.0848232182589443\n",
            "\n",
            "Epoch : 60 Test Loss : 1.0352233052253723, Test Accuracy : 0.674074074074074\n",
            "Epoch : 61\n",
            "Train loss : 1.090212 [0/700]\n",
            "Train loss : 0.938175 [64/700]\n",
            "Train loss : 0.965994 [128/700]\n",
            "Train loss : 1.311836 [192/700]\n",
            "Train loss : 1.043623 [256/700]\n",
            "Train loss : 1.113733 [320/700]\n",
            "Train loss : 1.168665 [384/700]\n",
            "Train loss : 1.047764 [448/700]\n",
            "Train loss : 1.065682 [512/700]\n",
            "Train loss : 1.079668 [576/700]\n",
            "Train loss : 1.051894 [600/700]\n",
            "Average Train loss : 1.079749665477059\n",
            "\n",
            "Epoch : 61 Test Loss : 1.0301316261291504, Test Accuracy : 0.674074074074074\n",
            "Epoch : 62\n",
            "Train loss : 1.162849 [0/700]\n",
            "Train loss : 0.987450 [64/700]\n",
            "Train loss : 1.031763 [128/700]\n",
            "Train loss : 1.016047 [192/700]\n",
            "Train loss : 1.129454 [256/700]\n",
            "Train loss : 1.138476 [320/700]\n",
            "Train loss : 0.896917 [384/700]\n",
            "Train loss : 1.056189 [448/700]\n",
            "Train loss : 1.208327 [512/700]\n",
            "Train loss : 1.164088 [576/700]\n",
            "Train loss : 1.031142 [600/700]\n",
            "Average Train loss : 1.074791133403778\n",
            "\n",
            "Epoch : 62 Test Loss : 1.024845802783966, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 63\n",
            "Train loss : 0.954167 [0/700]\n",
            "Train loss : 0.960552 [64/700]\n",
            "Train loss : 0.972131 [128/700]\n",
            "Train loss : 1.155913 [192/700]\n",
            "Train loss : 1.266952 [256/700]\n",
            "Train loss : 1.066974 [320/700]\n",
            "Train loss : 1.073203 [384/700]\n",
            "Train loss : 1.121900 [448/700]\n",
            "Train loss : 1.054130 [512/700]\n",
            "Train loss : 1.069568 [576/700]\n",
            "Train loss : 1.077389 [600/700]\n",
            "Average Train loss : 1.0702619281682102\n",
            "\n",
            "Epoch : 63 Test Loss : 1.0200856566429137, Test Accuracy : 0.674074074074074\n",
            "Epoch : 64\n",
            "Train loss : 1.046029 [0/700]\n",
            "Train loss : 1.167988 [64/700]\n",
            "Train loss : 1.037125 [128/700]\n",
            "Train loss : 0.982086 [192/700]\n",
            "Train loss : 1.122406 [256/700]\n",
            "Train loss : 0.982772 [320/700]\n",
            "Train loss : 1.086412 [384/700]\n",
            "Train loss : 1.112304 [448/700]\n",
            "Train loss : 0.977799 [512/700]\n",
            "Train loss : 1.139793 [576/700]\n",
            "Train loss : 1.065028 [600/700]\n",
            "Average Train loss : 1.065431080081246\n",
            "\n",
            "Epoch : 64 Test Loss : 1.015489685535431, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 65\n",
            "Train loss : 1.159654 [0/700]\n",
            "Train loss : 1.218279 [64/700]\n",
            "Train loss : 1.015190 [128/700]\n",
            "Train loss : 1.024530 [192/700]\n",
            "Train loss : 1.057391 [256/700]\n",
            "Train loss : 0.963408 [320/700]\n",
            "Train loss : 1.144958 [384/700]\n",
            "Train loss : 1.067755 [448/700]\n",
            "Train loss : 1.080476 [512/700]\n",
            "Train loss : 0.965228 [576/700]\n",
            "Train loss : 0.967047 [600/700]\n",
            "Average Train loss : 1.0603559667413884\n",
            "\n",
            "Epoch : 65 Test Loss : 1.0109808921813965, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 66\n",
            "Train loss : 1.138025 [0/700]\n",
            "Train loss : 1.139318 [64/700]\n",
            "Train loss : 0.990253 [128/700]\n",
            "Train loss : 0.992380 [192/700]\n",
            "Train loss : 0.931748 [256/700]\n",
            "Train loss : 1.235667 [320/700]\n",
            "Train loss : 1.129692 [384/700]\n",
            "Train loss : 1.012101 [448/700]\n",
            "Train loss : 0.914271 [512/700]\n",
            "Train loss : 0.969258 [576/700]\n",
            "Train loss : 1.172093 [600/700]\n",
            "Average Train loss : 1.0568005550991406\n",
            "\n",
            "Epoch : 66 Test Loss : 1.0062063455581665, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 67\n",
            "Train loss : 1.055662 [0/700]\n",
            "Train loss : 0.984461 [64/700]\n",
            "Train loss : 1.155549 [128/700]\n",
            "Train loss : 0.987239 [192/700]\n",
            "Train loss : 1.055227 [256/700]\n",
            "Train loss : 1.101233 [320/700]\n",
            "Train loss : 1.211787 [384/700]\n",
            "Train loss : 1.008624 [448/700]\n",
            "Train loss : 0.864335 [512/700]\n",
            "Train loss : 0.987929 [576/700]\n",
            "Train loss : 1.161862 [600/700]\n",
            "Average Train loss : 1.0521736145019531\n",
            "\n",
            "Epoch : 67 Test Loss : 1.0014727711677551, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 68\n",
            "Train loss : 1.098588 [0/700]\n",
            "Train loss : 0.923199 [64/700]\n",
            "Train loss : 1.027135 [128/700]\n",
            "Train loss : 1.054080 [192/700]\n",
            "Train loss : 1.114939 [256/700]\n",
            "Train loss : 1.018246 [320/700]\n",
            "Train loss : 1.005468 [384/700]\n",
            "Train loss : 1.166407 [448/700]\n",
            "Train loss : 0.941473 [512/700]\n",
            "Train loss : 1.111348 [576/700]\n",
            "Train loss : 1.057083 [600/700]\n",
            "Average Train loss : 1.0470878590237012\n",
            "\n",
            "Epoch : 68 Test Loss : 0.9971523284912109, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 69\n",
            "Train loss : 1.025974 [0/700]\n",
            "Train loss : 1.036532 [64/700]\n",
            "Train loss : 1.018961 [128/700]\n",
            "Train loss : 1.107164 [192/700]\n",
            "Train loss : 1.018822 [256/700]\n",
            "Train loss : 1.014193 [320/700]\n",
            "Train loss : 0.996363 [384/700]\n",
            "Train loss : 1.012518 [448/700]\n",
            "Train loss : 1.118893 [512/700]\n",
            "Train loss : 1.201052 [576/700]\n",
            "Train loss : 0.910944 [600/700]\n",
            "Average Train loss : 1.0419471101327376\n",
            "\n",
            "Epoch : 69 Test Loss : 0.9927839875221253, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 70\n",
            "Train loss : 0.925432 [0/700]\n",
            "Train loss : 1.008010 [64/700]\n",
            "Train loss : 1.089308 [128/700]\n",
            "Train loss : 1.071941 [192/700]\n",
            "Train loss : 1.109989 [256/700]\n",
            "Train loss : 1.125894 [320/700]\n",
            "Train loss : 1.151049 [384/700]\n",
            "Train loss : 1.010120 [448/700]\n",
            "Train loss : 0.972776 [512/700]\n",
            "Train loss : 0.974119 [576/700]\n",
            "Train loss : 0.980157 [600/700]\n",
            "Average Train loss : 1.0380722175944934\n",
            "\n",
            "Epoch : 70 Test Loss : 0.988773238658905, Test Accuracy : 0.6777777777777778\n",
            "Epoch : 71\n",
            "Train loss : 1.071773 [0/700]\n",
            "Train loss : 1.152327 [64/700]\n",
            "Train loss : 1.048050 [128/700]\n",
            "Train loss : 0.957023 [192/700]\n",
            "Train loss : 1.185791 [256/700]\n",
            "Train loss : 1.022684 [320/700]\n",
            "Train loss : 0.769917 [384/700]\n",
            "Train loss : 1.008581 [448/700]\n",
            "Train loss : 1.155649 [512/700]\n",
            "Train loss : 1.088808 [576/700]\n",
            "Train loss : 0.906929 [600/700]\n",
            "Average Train loss : 1.0334120880473743\n",
            "\n",
            "Epoch : 71 Test Loss : 0.9841486573219299, Test Accuracy : 0.6814814814814815\n",
            "Epoch : 72\n",
            "Train loss : 1.069155 [0/700]\n",
            "Train loss : 1.029323 [64/700]\n",
            "Train loss : 1.052914 [128/700]\n",
            "Train loss : 1.098938 [192/700]\n",
            "Train loss : 1.075867 [256/700]\n",
            "Train loss : 0.944505 [320/700]\n",
            "Train loss : 1.009555 [384/700]\n",
            "Train loss : 1.095360 [448/700]\n",
            "Train loss : 0.900742 [512/700]\n",
            "Train loss : 0.947572 [576/700]\n",
            "Train loss : 1.109413 [600/700]\n",
            "Average Train loss : 1.0303039875897495\n",
            "\n",
            "Epoch : 72 Test Loss : 0.9801409125328064, Test Accuracy : 0.6814814814814815\n",
            "Epoch : 73\n",
            "Train loss : 0.969649 [0/700]\n",
            "Train loss : 0.967275 [64/700]\n",
            "Train loss : 1.071087 [128/700]\n",
            "Train loss : 1.081186 [192/700]\n",
            "Train loss : 0.931213 [256/700]\n",
            "Train loss : 1.103674 [320/700]\n",
            "Train loss : 1.045065 [384/700]\n",
            "Train loss : 1.026635 [448/700]\n",
            "Train loss : 0.956532 [512/700]\n",
            "Train loss : 0.968440 [576/700]\n",
            "Train loss : 1.170419 [600/700]\n",
            "Average Train loss : 1.026470428163355\n",
            "\n",
            "Epoch : 73 Test Loss : 0.9763601422309875, Test Accuracy : 0.6851851851851852\n",
            "Epoch : 74\n",
            "Train loss : 1.057588 [0/700]\n",
            "Train loss : 0.884528 [64/700]\n",
            "Train loss : 0.983725 [128/700]\n",
            "Train loss : 1.066167 [192/700]\n",
            "Train loss : 0.938546 [256/700]\n",
            "Train loss : 0.893563 [320/700]\n",
            "Train loss : 1.134661 [384/700]\n",
            "Train loss : 1.033792 [448/700]\n",
            "Train loss : 1.100379 [512/700]\n",
            "Train loss : 1.060437 [576/700]\n",
            "Train loss : 1.088932 [600/700]\n",
            "Average Train loss : 1.0220287984067744\n",
            "\n",
            "Epoch : 74 Test Loss : 0.9720552802085877, Test Accuracy : 0.6851851851851852\n",
            "Epoch : 75\n",
            "Train loss : 1.043517 [0/700]\n",
            "Train loss : 1.027915 [64/700]\n",
            "Train loss : 0.954386 [128/700]\n",
            "Train loss : 0.980213 [192/700]\n",
            "Train loss : 0.897194 [256/700]\n",
            "Train loss : 1.060386 [320/700]\n",
            "Train loss : 1.066119 [384/700]\n",
            "Train loss : 1.062290 [448/700]\n",
            "Train loss : 0.991388 [512/700]\n",
            "Train loss : 1.123481 [576/700]\n",
            "Train loss : 0.984989 [600/700]\n",
            "Average Train loss : 1.017443526874889\n",
            "\n",
            "Epoch : 75 Test Loss : 0.968133008480072, Test Accuracy : 0.6851851851851852\n",
            "Epoch : 76\n",
            "Train loss : 0.984711 [0/700]\n",
            "Train loss : 1.042491 [64/700]\n",
            "Train loss : 0.913559 [128/700]\n",
            "Train loss : 1.185756 [192/700]\n",
            "Train loss : 0.891084 [256/700]\n",
            "Train loss : 1.023303 [320/700]\n",
            "Train loss : 0.990983 [384/700]\n",
            "Train loss : 1.067100 [448/700]\n",
            "Train loss : 1.068650 [512/700]\n",
            "Train loss : 0.988324 [576/700]\n",
            "Train loss : 0.992256 [600/700]\n",
            "Average Train loss : 1.0134743126955899\n",
            "\n",
            "Epoch : 76 Test Loss : 0.9640316843986512, Test Accuracy : 0.6851851851851852\n",
            "Epoch : 77\n",
            "Train loss : 0.960272 [0/700]\n",
            "Train loss : 0.992813 [64/700]\n",
            "Train loss : 0.896080 [128/700]\n",
            "Train loss : 1.057544 [192/700]\n",
            "Train loss : 0.908810 [256/700]\n",
            "Train loss : 1.101071 [320/700]\n",
            "Train loss : 1.091885 [384/700]\n",
            "Train loss : 0.952211 [448/700]\n",
            "Train loss : 1.088083 [512/700]\n",
            "Train loss : 1.000681 [576/700]\n",
            "Train loss : 1.059541 [600/700]\n",
            "Average Train loss : 1.0099082697521558\n",
            "\n",
            "Epoch : 77 Test Loss : 0.9602656722068786, Test Accuracy : 0.6888888888888889\n",
            "Epoch : 78\n",
            "Train loss : 0.927819 [0/700]\n",
            "Train loss : 1.068371 [64/700]\n",
            "Train loss : 1.019771 [128/700]\n",
            "Train loss : 1.082122 [192/700]\n",
            "Train loss : 0.890945 [256/700]\n",
            "Train loss : 0.915123 [320/700]\n",
            "Train loss : 0.937739 [384/700]\n",
            "Train loss : 1.018981 [448/700]\n",
            "Train loss : 1.094847 [512/700]\n",
            "Train loss : 1.195838 [576/700]\n",
            "Train loss : 0.905620 [600/700]\n",
            "Average Train loss : 1.0051978555592624\n",
            "\n",
            "Epoch : 78 Test Loss : 0.9566999197006225, Test Accuracy : 0.6888888888888889\n",
            "Epoch : 79\n",
            "Train loss : 1.035076 [0/700]\n",
            "Train loss : 1.072693 [64/700]\n",
            "Train loss : 1.018508 [128/700]\n",
            "Train loss : 0.992656 [192/700]\n",
            "Train loss : 1.113574 [256/700]\n",
            "Train loss : 0.994236 [320/700]\n",
            "Train loss : 0.918429 [384/700]\n",
            "Train loss : 0.931508 [448/700]\n",
            "Train loss : 1.008250 [512/700]\n",
            "Train loss : 0.959004 [576/700]\n",
            "Train loss : 0.976458 [600/700]\n",
            "Average Train loss : 1.0018538670106367\n",
            "\n",
            "Epoch : 79 Test Loss : 0.9527939319610595, Test Accuracy : 0.6888888888888889\n",
            "Epoch : 80\n",
            "Train loss : 1.042259 [0/700]\n",
            "Train loss : 0.987730 [64/700]\n",
            "Train loss : 1.155993 [128/700]\n",
            "Train loss : 1.001087 [192/700]\n",
            "Train loss : 0.918430 [256/700]\n",
            "Train loss : 0.961235 [320/700]\n",
            "Train loss : 1.043680 [384/700]\n",
            "Train loss : 0.897814 [448/700]\n",
            "Train loss : 0.964712 [512/700]\n",
            "Train loss : 1.001896 [576/700]\n",
            "Train loss : 1.005597 [600/700]\n",
            "Average Train loss : 0.9982212077487599\n",
            "\n",
            "Epoch : 80 Test Loss : 0.9492233395576477, Test Accuracy : 0.6925925925925925\n",
            "Epoch : 81\n",
            "Train loss : 0.995927 [0/700]\n",
            "Train loss : 0.915426 [64/700]\n",
            "Train loss : 1.108842 [128/700]\n",
            "Train loss : 0.912073 [192/700]\n",
            "Train loss : 1.112706 [256/700]\n",
            "Train loss : 0.942426 [320/700]\n",
            "Train loss : 1.015756 [384/700]\n",
            "Train loss : 0.994070 [448/700]\n",
            "Train loss : 0.982818 [512/700]\n",
            "Train loss : 0.920462 [576/700]\n",
            "Train loss : 1.041154 [600/700]\n",
            "Average Train loss : 0.9946963732892816\n",
            "\n",
            "Epoch : 81 Test Loss : 0.9453466892242431, Test Accuracy : 0.6888888888888889\n",
            "Epoch : 82\n",
            "Train loss : 1.157508 [0/700]\n",
            "Train loss : 1.189959 [64/700]\n",
            "Train loss : 0.853850 [128/700]\n",
            "Train loss : 0.979560 [192/700]\n",
            "Train loss : 0.949849 [256/700]\n",
            "Train loss : 0.960978 [320/700]\n",
            "Train loss : 0.938675 [384/700]\n",
            "Train loss : 1.012074 [448/700]\n",
            "Train loss : 0.943675 [512/700]\n",
            "Train loss : 0.906120 [576/700]\n",
            "Train loss : 1.006660 [600/700]\n",
            "Average Train loss : 0.9908099228685553\n",
            "\n",
            "Epoch : 82 Test Loss : 0.9417190551757812, Test Accuracy : 0.6888888888888889\n",
            "Epoch : 83\n",
            "Train loss : 0.998834 [0/700]\n",
            "Train loss : 0.881263 [64/700]\n",
            "Train loss : 0.972263 [128/700]\n",
            "Train loss : 0.867110 [192/700]\n",
            "Train loss : 0.980033 [256/700]\n",
            "Train loss : 1.069874 [320/700]\n",
            "Train loss : 0.998475 [384/700]\n",
            "Train loss : 0.986580 [448/700]\n",
            "Train loss : 1.054863 [512/700]\n",
            "Train loss : 0.980997 [576/700]\n",
            "Train loss : 1.073397 [600/700]\n",
            "Average Train loss : 0.987608091397719\n",
            "\n",
            "Epoch : 83 Test Loss : 0.9381078839302063, Test Accuracy : 0.6851851851851852\n",
            "Epoch : 84\n",
            "Train loss : 0.921746 [0/700]\n",
            "Train loss : 0.983279 [64/700]\n",
            "Train loss : 1.108287 [128/700]\n",
            "Train loss : 0.849203 [192/700]\n",
            "Train loss : 1.038902 [256/700]\n",
            "Train loss : 1.163447 [320/700]\n",
            "Train loss : 1.094362 [384/700]\n",
            "Train loss : 0.842669 [448/700]\n",
            "Train loss : 0.991908 [512/700]\n",
            "Train loss : 0.835288 [576/700]\n",
            "Train loss : 0.989308 [600/700]\n",
            "Average Train loss : 0.9834907271645286\n",
            "\n",
            "Epoch : 84 Test Loss : 0.9346948027610779, Test Accuracy : 0.6888888888888889\n",
            "Epoch : 85\n",
            "Train loss : 1.080885 [0/700]\n",
            "Train loss : 1.139519 [64/700]\n",
            "Train loss : 1.178074 [128/700]\n",
            "Train loss : 0.994802 [192/700]\n",
            "Train loss : 0.989719 [256/700]\n",
            "Train loss : 0.906676 [320/700]\n",
            "Train loss : 0.810787 [384/700]\n",
            "Train loss : 0.883497 [448/700]\n",
            "Train loss : 0.880564 [512/700]\n",
            "Train loss : 0.920721 [576/700]\n",
            "Train loss : 0.996158 [600/700]\n",
            "Average Train loss : 0.9801274646412242\n",
            "\n",
            "Epoch : 85 Test Loss : 0.9312714457511901, Test Accuracy : 0.6925925925925925\n",
            "Epoch : 86\n",
            "Train loss : 1.039640 [0/700]\n",
            "Train loss : 0.977961 [64/700]\n",
            "Train loss : 0.913817 [128/700]\n",
            "Train loss : 1.032586 [192/700]\n",
            "Train loss : 1.099398 [256/700]\n",
            "Train loss : 1.015331 [320/700]\n",
            "Train loss : 0.864562 [384/700]\n",
            "Train loss : 0.972068 [448/700]\n",
            "Train loss : 0.944805 [512/700]\n",
            "Train loss : 0.921746 [576/700]\n",
            "Train loss : 0.957174 [600/700]\n",
            "Average Train loss : 0.9762807380069386\n",
            "\n",
            "Epoch : 86 Test Loss : 0.9276872754096985, Test Accuracy : 0.7\n",
            "Epoch : 87\n",
            "Train loss : 1.030648 [0/700]\n",
            "Train loss : 0.971175 [64/700]\n",
            "Train loss : 1.006882 [128/700]\n",
            "Train loss : 0.944878 [192/700]\n",
            "Train loss : 0.967658 [256/700]\n",
            "Train loss : 0.902383 [320/700]\n",
            "Train loss : 0.944675 [384/700]\n",
            "Train loss : 0.974228 [448/700]\n",
            "Train loss : 1.061358 [512/700]\n",
            "Train loss : 0.981119 [576/700]\n",
            "Train loss : 0.913204 [600/700]\n",
            "Average Train loss : 0.9725644046610052\n",
            "\n",
            "Epoch : 87 Test Loss : 0.9241186618804932, Test Accuracy : 0.6962962962962963\n",
            "Epoch : 88\n",
            "Train loss : 1.029218 [0/700]\n",
            "Train loss : 0.896429 [64/700]\n",
            "Train loss : 0.960218 [128/700]\n",
            "Train loss : 0.984030 [192/700]\n",
            "Train loss : 0.927909 [256/700]\n",
            "Train loss : 0.861359 [320/700]\n",
            "Train loss : 0.901920 [384/700]\n",
            "Train loss : 0.990911 [448/700]\n",
            "Train loss : 1.231772 [512/700]\n",
            "Train loss : 1.023613 [576/700]\n",
            "Train loss : 0.848753 [600/700]\n",
            "Average Train loss : 0.9687392928383567\n",
            "\n",
            "Epoch : 88 Test Loss : 0.9208944797515869, Test Accuracy : 0.6962962962962963\n",
            "Epoch : 89\n",
            "Train loss : 1.028549 [0/700]\n",
            "Train loss : 0.874437 [64/700]\n",
            "Train loss : 0.921426 [128/700]\n",
            "Train loss : 0.961802 [192/700]\n",
            "Train loss : 0.817270 [256/700]\n",
            "Train loss : 0.984141 [320/700]\n",
            "Train loss : 1.052953 [384/700]\n",
            "Train loss : 1.231711 [448/700]\n",
            "Train loss : 0.905371 [512/700]\n",
            "Train loss : 0.974217 [576/700]\n",
            "Train loss : 0.868099 [600/700]\n",
            "Average Train loss : 0.9654523676091974\n",
            "\n",
            "Epoch : 89 Test Loss : 0.9176114559173584, Test Accuracy : 0.6962962962962963\n",
            "Epoch : 90\n",
            "Train loss : 0.882923 [0/700]\n",
            "Train loss : 1.097377 [64/700]\n",
            "Train loss : 0.956276 [128/700]\n",
            "Train loss : 0.887757 [192/700]\n",
            "Train loss : 0.887695 [256/700]\n",
            "Train loss : 1.099731 [320/700]\n",
            "Train loss : 1.018488 [384/700]\n",
            "Train loss : 1.135392 [448/700]\n",
            "Train loss : 0.930799 [512/700]\n",
            "Train loss : 0.856290 [576/700]\n",
            "Train loss : 0.828137 [600/700]\n",
            "Average Train loss : 0.9618968530134722\n",
            "\n",
            "Epoch : 90 Test Loss : 0.9143862962722779, Test Accuracy : 0.7\n",
            "Epoch : 91\n",
            "Train loss : 0.860394 [0/700]\n",
            "Train loss : 0.844225 [64/700]\n",
            "Train loss : 1.038890 [128/700]\n",
            "Train loss : 0.967996 [192/700]\n",
            "Train loss : 0.964754 [256/700]\n",
            "Train loss : 0.893693 [320/700]\n",
            "Train loss : 0.882945 [384/700]\n",
            "Train loss : 1.001527 [448/700]\n",
            "Train loss : 1.027171 [512/700]\n",
            "Train loss : 1.024064 [576/700]\n",
            "Train loss : 1.052459 [600/700]\n",
            "Average Train loss : 0.9598288915374062\n",
            "\n",
            "Epoch : 91 Test Loss : 0.9110809803009033, Test Accuracy : 0.7\n",
            "Epoch : 92\n",
            "Train loss : 0.923283 [0/700]\n",
            "Train loss : 1.001808 [64/700]\n",
            "Train loss : 0.995260 [128/700]\n",
            "Train loss : 0.915121 [192/700]\n",
            "Train loss : 0.919535 [256/700]\n",
            "Train loss : 1.079913 [320/700]\n",
            "Train loss : 0.945239 [384/700]\n",
            "Train loss : 0.942662 [448/700]\n",
            "Train loss : 0.887214 [512/700]\n",
            "Train loss : 0.959819 [576/700]\n",
            "Train loss : 0.945167 [600/700]\n",
            "Average Train loss : 0.9559110999107361\n",
            "\n",
            "Epoch : 92 Test Loss : 0.9078553199768067, Test Accuracy : 0.7\n",
            "Epoch : 93\n",
            "Train loss : 0.859743 [0/700]\n",
            "Train loss : 1.007732 [64/700]\n",
            "Train loss : 0.859399 [128/700]\n",
            "Train loss : 0.990699 [192/700]\n",
            "Train loss : 0.929355 [256/700]\n",
            "Train loss : 0.967248 [320/700]\n",
            "Train loss : 0.990150 [384/700]\n",
            "Train loss : 0.810286 [448/700]\n",
            "Train loss : 1.009611 [512/700]\n",
            "Train loss : 1.033770 [576/700]\n",
            "Train loss : 1.026544 [600/700]\n",
            "Average Train loss : 0.9531398415565491\n",
            "\n",
            "Epoch : 93 Test Loss : 0.9045949816703797, Test Accuracy : 0.7037037037037037\n",
            "Epoch : 94\n",
            "Train loss : 0.929790 [0/700]\n",
            "Train loss : 0.907885 [64/700]\n",
            "Train loss : 1.008439 [128/700]\n",
            "Train loss : 0.904741 [192/700]\n",
            "Train loss : 1.016752 [256/700]\n",
            "Train loss : 1.056998 [320/700]\n",
            "Train loss : 0.981100 [384/700]\n",
            "Train loss : 0.959458 [448/700]\n",
            "Train loss : 0.995924 [512/700]\n",
            "Train loss : 0.810691 [576/700]\n",
            "Train loss : 0.867588 [600/700]\n",
            "Average Train loss : 0.9490333470431241\n",
            "\n",
            "Epoch : 94 Test Loss : 0.9011546492576599, Test Accuracy : 0.7074074074074074\n",
            "Epoch : 95\n",
            "Train loss : 0.933833 [0/700]\n",
            "Train loss : 0.935712 [64/700]\n",
            "Train loss : 0.920305 [128/700]\n",
            "Train loss : 0.971446 [192/700]\n",
            "Train loss : 1.054781 [256/700]\n",
            "Train loss : 0.964508 [320/700]\n",
            "Train loss : 0.852366 [384/700]\n",
            "Train loss : 0.945546 [448/700]\n",
            "Train loss : 0.980825 [512/700]\n",
            "Train loss : 0.949916 [576/700]\n",
            "Train loss : 0.896668 [600/700]\n",
            "Average Train loss : 0.945991418578408\n",
            "\n",
            "Epoch : 95 Test Loss : 0.8981973886489868, Test Accuracy : 0.7037037037037037\n",
            "Epoch : 96\n",
            "Train loss : 0.986363 [0/700]\n",
            "Train loss : 0.907759 [64/700]\n",
            "Train loss : 1.004513 [128/700]\n",
            "Train loss : 0.828572 [192/700]\n",
            "Train loss : 1.043893 [256/700]\n",
            "Train loss : 0.940060 [320/700]\n",
            "Train loss : 0.946724 [384/700]\n",
            "Train loss : 0.965873 [448/700]\n",
            "Train loss : 0.995516 [512/700]\n",
            "Train loss : 0.828054 [576/700]\n",
            "Train loss : 0.925733 [600/700]\n",
            "Average Train loss : 0.9430054534565319\n",
            "\n",
            "Epoch : 96 Test Loss : 0.894905424118042, Test Accuracy : 0.7074074074074074\n",
            "Epoch : 97\n",
            "Train loss : 1.010398 [0/700]\n",
            "Train loss : 0.864173 [64/700]\n",
            "Train loss : 1.005140 [128/700]\n",
            "Train loss : 0.967788 [192/700]\n",
            "Train loss : 0.867831 [256/700]\n",
            "Train loss : 0.897345 [320/700]\n",
            "Train loss : 0.992153 [384/700]\n",
            "Train loss : 0.915558 [448/700]\n",
            "Train loss : 0.958831 [512/700]\n",
            "Train loss : 0.851913 [576/700]\n",
            "Train loss : 1.012393 [600/700]\n",
            "Average Train loss : 0.9403203346512534\n",
            "\n",
            "Epoch : 97 Test Loss : 0.8919144630432129, Test Accuracy : 0.7037037037037037\n",
            "Epoch : 98\n",
            "Train loss : 0.994466 [0/700]\n",
            "Train loss : 0.918801 [64/700]\n",
            "Train loss : 1.019080 [128/700]\n",
            "Train loss : 1.013686 [192/700]\n",
            "Train loss : 0.998967 [256/700]\n",
            "Train loss : 0.930783 [320/700]\n",
            "Train loss : 0.956242 [384/700]\n",
            "Train loss : 0.837756 [448/700]\n",
            "Train loss : 0.859252 [512/700]\n",
            "Train loss : 0.854009 [576/700]\n",
            "Train loss : 0.921118 [600/700]\n",
            "Average Train loss : 0.9367418776858937\n",
            "\n",
            "Epoch : 98 Test Loss : 0.8888927221298217, Test Accuracy : 0.7037037037037037\n",
            "Epoch : 99\n",
            "Train loss : 0.915595 [0/700]\n",
            "Train loss : 0.782416 [64/700]\n",
            "Train loss : 0.802239 [128/700]\n",
            "Train loss : 0.945558 [192/700]\n",
            "Train loss : 1.013708 [256/700]\n",
            "Train loss : 0.981444 [320/700]\n",
            "Train loss : 0.948146 [384/700]\n",
            "Train loss : 1.060189 [448/700]\n",
            "Train loss : 0.928867 [512/700]\n",
            "Train loss : 1.005024 [576/700]\n",
            "Train loss : 0.884743 [600/700]\n",
            "Average Train loss : 0.9334481087597933\n",
            "\n",
            "Epoch : 99 Test Loss : 0.8856472253799439, Test Accuracy : 0.7037037037037037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 조기 종료"
      ],
      "metadata": {
        "id": "jqnfVHbPY_TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 99999\n",
        "counter = 0\n",
        "for epoch in range(100):\n",
        "  train(model,train_loader,optimizer,criterion,epoch)\n",
        "  current_loss = test(model, valid_loader, criterion,epoch)\n",
        "  if best_loss >= current_loss:\n",
        "    best_loss = current_loss\n",
        "    counter += 1\n",
        "    if counter == 1:\n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOFlDJXJY6Vk",
        "outputId": "410b1902-97f0-494d-d368-3254178daf9c"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "Train loss : 0.886606 [0/700]\n",
            "Train loss : 0.942702 [64/700]\n",
            "Train loss : 0.910273 [128/700]\n",
            "Train loss : 0.976576 [192/700]\n",
            "Train loss : 0.920570 [256/700]\n",
            "Train loss : 0.842202 [320/700]\n",
            "Train loss : 1.056989 [384/700]\n",
            "Train loss : 0.888512 [448/700]\n",
            "Train loss : 0.874069 [512/700]\n",
            "Train loss : 0.920043 [576/700]\n",
            "Train loss : 1.024636 [600/700]\n",
            "Average Train loss : 0.9311979521404613\n",
            "\n",
            "Epoch : 0 Test Loss : 0.8827088952064515, Test Accuracy : 0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모형 저장"
      ],
      "metadata": {
        "id": "a2mSU5OGZftr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "0LAfsJ4HZi1A"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 99999\n",
        "counter = 0\n",
        "best_model_weight = copy.deepcopy(model.state_dict())\n",
        "for epoch in range(100):\n",
        "  train(model,train_loader,optimizer,criterion,epoch)\n",
        "  current_loss = test(model, valid_loader, criterion,epoch)\n",
        "  if best_loss >= current_loss:\n",
        "    best_loss = current_loss\n",
        "    counter += 1\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if counter == 1:\n",
        "      break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmkxIpYJZcUY",
        "outputId": "08de8987-28f4-4dce-ab4e-7be8939918b7"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0\n",
            "Train loss : 0.897579 [0/700]\n",
            "Train loss : 0.965448 [64/700]\n",
            "Train loss : 0.903775 [128/700]\n",
            "Train loss : 0.829314 [192/700]\n",
            "Train loss : 1.072744 [256/700]\n",
            "Train loss : 0.902160 [320/700]\n",
            "Train loss : 0.883274 [384/700]\n",
            "Train loss : 0.892202 [448/700]\n",
            "Train loss : 0.938664 [512/700]\n",
            "Train loss : 1.110027 [576/700]\n",
            "Train loss : 0.766233 [600/700]\n",
            "Average Train loss : 0.9237655401229858\n",
            "\n",
            "Epoch : 0 Test Loss : 0.8768467783927918, Test Accuracy : 0.7037037037037037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJTPn4B8abkc",
        "outputId": "a9b44b76-af7a-498e-be43-750304fe09bf"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight', tensor([[-0.0010,  0.5596],\n",
              "                      [ 0.7757,  0.5144],\n",
              "                      [ 0.5669, -0.4354],\n",
              "                      [-0.1434, -0.0073],\n",
              "                      [-0.4750,  0.4301],\n",
              "                      [ 0.1007, -0.5178],\n",
              "                      [ 0.3856, -0.5476],\n",
              "                      [-0.5501,  0.6045],\n",
              "                      [-0.7784, -0.3684],\n",
              "                      [ 0.5911, -0.0483],\n",
              "                      [-0.3847, -0.3516],\n",
              "                      [ 0.1852,  0.4514],\n",
              "                      [-0.4671,  0.6305],\n",
              "                      [-0.6958, -0.0932],\n",
              "                      [ 0.6549, -0.1369],\n",
              "                      [ 0.0239, -0.1662],\n",
              "                      [-0.3578, -0.1628],\n",
              "                      [-0.6937, -0.2579],\n",
              "                      [ 0.5996, -0.4211],\n",
              "                      [ 0.5521,  0.1099],\n",
              "                      [ 0.7470, -0.3141],\n",
              "                      [ 0.1953,  0.2552],\n",
              "                      [-0.7037,  0.7341],\n",
              "                      [-0.5113, -0.3807],\n",
              "                      [-0.3969, -0.4813],\n",
              "                      [ 0.5724,  0.3231],\n",
              "                      [ 0.2584,  0.7293],\n",
              "                      [-0.0752,  0.4678],\n",
              "                      [ 0.2414,  0.6119],\n",
              "                      [ 0.7189, -0.0166],\n",
              "                      [-0.7358,  0.3912],\n",
              "                      [-0.4223,  0.1299],\n",
              "                      [ 0.7159,  0.4142],\n",
              "                      [ 0.5823, -0.0407],\n",
              "                      [ 0.3407,  0.0153],\n",
              "                      [-0.3028, -0.1911],\n",
              "                      [-0.1290,  0.1892],\n",
              "                      [ 0.3207, -0.4374],\n",
              "                      [ 0.2170, -0.2373],\n",
              "                      [-0.6612,  0.0823],\n",
              "                      [-0.3625,  0.3515],\n",
              "                      [ 0.7132, -0.2038],\n",
              "                      [-0.7659,  0.2711],\n",
              "                      [ 0.4610,  0.4500],\n",
              "                      [ 0.4790, -0.0013],\n",
              "                      [ 0.0978,  0.4564],\n",
              "                      [ 0.0491, -0.6017],\n",
              "                      [-0.3679,  0.2441],\n",
              "                      [-0.4252, -0.3948],\n",
              "                      [ 0.5316, -0.5319]], device='cuda:0')),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.6653,  0.1816,  0.2804, -0.1701, -0.1798,  0.2997,  0.6486,  0.3188,\n",
              "                       0.2868,  0.6250,  0.0709, -0.2623, -0.2155, -0.4560, -0.5402, -0.0768,\n",
              "                      -0.0197,  0.0874,  0.5543,  0.1997,  0.5654,  0.2261,  0.2241, -0.6697,\n",
              "                      -0.4209, -0.5717,  0.0010,  0.3425, -0.1977, -0.5729,  0.6308,  0.3584,\n",
              "                      -0.5757, -0.2814,  0.2078,  0.8333, -0.4299, -0.0748,  0.6045, -0.3616,\n",
              "                      -0.0302, -0.6545,  0.5668,  0.4498, -0.2449,  0.4377, -0.4153,  0.4165,\n",
              "                       0.0448,  0.2375], device='cuda:0')),\n",
              "             ('out.weight',\n",
              "              tensor([[-3.3925e-02,  1.4995e-01, -1.6293e-01,  6.3326e-03,  4.6510e-02,\n",
              "                        1.5045e-02, -6.3035e-02,  6.8090e-02, -1.6994e-01,  9.0016e-03,\n",
              "                        8.0657e-02, -8.9743e-02,  9.8898e-02, -1.5030e-01, -3.4968e-03,\n",
              "                        4.5123e-02, -1.9112e-01,  4.9951e-02,  4.1470e-02, -2.4593e-02,\n",
              "                       -1.4439e-01, -7.2831e-02,  4.0335e-02, -1.3336e-01, -2.0284e-02,\n",
              "                        1.2014e-01,  6.1749e-02,  1.1887e-02, -8.0541e-02,  4.5427e-03,\n",
              "                        2.6741e-02, -1.1747e-01, -3.2035e-02, -1.6809e-01,  1.6836e-01,\n",
              "                       -1.9515e-01, -2.3299e-01, -5.1395e-02, -1.4026e-01, -4.4535e-02,\n",
              "                        6.5405e-02, -1.2163e-01, -1.1237e-01,  8.3752e-02, -1.2319e-01,\n",
              "                        1.5538e-01, -1.5469e-01, -3.9116e-02,  6.4166e-03, -4.2053e-02],\n",
              "                      [ 2.9124e-02,  7.4501e-02,  1.0366e-01,  3.4933e-02, -1.7753e-01,\n",
              "                        1.0194e-01, -5.8878e-02, -1.4670e-01, -1.6855e-01,  8.3521e-02,\n",
              "                       -1.5823e-01, -3.3758e-02,  6.1484e-02,  3.7709e-02, -1.1221e-02,\n",
              "                        5.3579e-02, -2.0562e-02,  1.8806e-02,  9.9288e-02, -5.1027e-02,\n",
              "                       -1.0068e-01,  1.1085e-01, -1.7682e-01, -1.0170e-01, -8.8874e-02,\n",
              "                        7.7656e-02, -2.3795e-02, -8.9569e-02, -3.5733e-02,  9.3795e-03,\n",
              "                        7.0761e-02, -1.0810e-01, -1.0390e-01,  3.5457e-02, -1.0790e-01,\n",
              "                        1.6706e-01, -9.9367e-02,  5.5050e-02,  5.9579e-02, -1.1478e-01,\n",
              "                        1.6719e-02, -7.9895e-02,  7.5053e-02,  8.9171e-02,  1.2454e-02,\n",
              "                        1.2042e-01, -5.7951e-02, -9.3389e-02,  4.3875e-02,  9.6712e-02],\n",
              "                      [-4.3366e-02,  4.2266e-02, -1.9751e-01, -2.3922e-01,  2.4937e-02,\n",
              "                       -1.5755e-01, -3.8263e-02, -5.3229e-02,  1.0753e-01,  3.6187e-03,\n",
              "                       -3.8318e-03,  4.0459e-02,  1.3285e-01, -1.2638e-01, -1.9681e-01,\n",
              "                       -1.9054e-01, -1.1089e-02, -1.9380e-02, -2.5931e-02,  5.3479e-02,\n",
              "                       -6.4723e-02, -5.0469e-02, -4.4900e-02, -1.0098e-01, -1.1896e-02,\n",
              "                        1.2775e-02, -4.5876e-02, -1.5517e-02, -1.5156e-01, -1.1871e-01,\n",
              "                        1.6232e-01, -3.4764e-02, -3.3542e-02, -2.0628e-01,  6.8815e-02,\n",
              "                        1.1497e-01, -1.9313e-01,  5.2552e-02, -8.8628e-02, -8.8121e-02,\n",
              "                        1.5717e-01, -5.2504e-02,  1.0330e-01, -6.3861e-02, -1.3924e-01,\n",
              "                        5.3626e-02, -2.1172e-01,  1.8181e-01, -5.5939e-02, -9.9484e-02],\n",
              "                      [ 9.1367e-02, -1.2826e-01, -9.0109e-02, -1.6957e-01, -2.6702e-02,\n",
              "                       -4.1490e-02,  3.5726e-02,  1.5400e-01, -1.9467e-01,  3.6795e-02,\n",
              "                       -1.4861e-01,  9.3067e-02,  1.5389e-01, -3.7511e-02, -2.1850e-01,\n",
              "                        6.0585e-02, -1.6559e-01, -1.9419e-01, -1.6636e-01, -1.0694e-02,\n",
              "                       -1.5366e-01, -1.8054e-02,  9.5159e-02,  3.1225e-02, -6.0668e-03,\n",
              "                        2.5248e-02,  8.1091e-02, -3.3020e-02, -5.0774e-02, -5.5184e-02,\n",
              "                        7.3069e-02, -4.7505e-02, -6.4864e-03, -4.1132e-02, -1.7861e-01,\n",
              "                       -6.4462e-03,  1.3208e-01, -2.4707e-02, -2.3063e-02,  3.7809e-02,\n",
              "                       -4.5363e-02, -2.0355e-01,  2.5722e-02,  4.9948e-02,  3.2211e-02,\n",
              "                        4.7944e-02, -1.3967e-01, -9.8928e-03, -2.0909e-01,  6.6502e-02],\n",
              "                      [ 3.5379e-02,  8.1864e-02,  1.8279e-01, -1.4409e-01, -1.2099e-01,\n",
              "                       -3.1518e-02, -1.3744e-02, -8.0850e-02, -6.5633e-02,  3.7234e-02,\n",
              "                        8.9507e-02, -1.7420e-01, -9.1552e-02, -1.5761e-02,  5.8588e-02,\n",
              "                       -4.2628e-02, -6.9275e-02,  4.8128e-02, -6.6202e-05, -6.4009e-02,\n",
              "                        1.5173e-01, -1.2710e-01, -1.3577e-01, -4.0373e-02, -1.1572e-01,\n",
              "                        6.0518e-02, -1.0494e-01, -1.1563e-01, -1.5412e-01, -5.8156e-02,\n",
              "                        2.1779e-02, -3.9828e-02,  1.0815e-01,  6.7860e-02,  1.6745e-01,\n",
              "                       -3.4287e-02, -1.6259e-01,  7.0945e-02,  1.4878e-01, -8.7322e-02,\n",
              "                       -1.0693e-01,  1.1805e-02,  1.3810e-02, -6.9549e-02,  2.3120e-02,\n",
              "                       -9.9297e-02,  2.3318e-01, -1.2422e-01, -1.7234e-01,  4.1249e-03],\n",
              "                      [-5.8088e-02,  1.4135e-01, -1.1510e-03, -1.6647e-01,  3.7814e-02,\n",
              "                        8.6595e-02,  1.8489e-03, -4.3088e-02, -3.6212e-02,  2.3436e-02,\n",
              "                        9.7269e-02, -1.6871e-01, -1.4569e-01, -7.3135e-02, -3.5150e-02,\n",
              "                       -1.0738e-01,  8.7810e-03, -1.6456e-01, -3.6415e-02,  1.1756e-01,\n",
              "                        1.6079e-02,  8.5778e-02, -1.9866e-01, -1.6009e-01,  7.6936e-02,\n",
              "                        1.6117e-01, -7.6310e-02, -2.2436e-01,  3.9443e-02,  1.0477e-01,\n",
              "                       -1.2389e-01, -2.0003e-01,  1.0765e-01,  6.5245e-02,  1.6197e-01,\n",
              "                        3.9634e-02, -5.5871e-02, -4.3455e-02, -5.7993e-02, -5.7646e-02,\n",
              "                       -1.2638e-01, -5.3403e-02, -1.0910e-01,  6.2114e-02,  5.5130e-03,\n",
              "                       -1.1774e-01, -6.7829e-03,  8.7981e-03, -3.0306e-02,  1.4381e-02],\n",
              "                      [-4.3453e-02, -3.2648e-02, -1.7083e-01, -1.4487e-01,  2.6473e-02,\n",
              "                       -3.9640e-02,  2.0712e-02,  4.0887e-02, -5.1364e-02,  4.0931e-02,\n",
              "                       -2.7310e-02,  5.8779e-02,  8.0681e-02,  2.7196e-02, -1.2340e-01,\n",
              "                       -2.7438e-02,  6.0313e-02, -1.1956e-01, -1.4132e-01,  5.1570e-02,\n",
              "                       -1.1646e-01,  3.3057e-02,  5.0068e-03, -1.2846e-01,  8.8415e-02,\n",
              "                        2.7556e-02,  7.1628e-02,  1.8966e-01, -2.3877e-02,  1.9934e-02,\n",
              "                        1.1641e-04, -8.7495e-02,  4.1531e-02, -7.0111e-02, -1.5954e-02,\n",
              "                       -1.5606e-01,  1.1744e-01,  2.4773e-02, -1.4749e-01, -1.4802e-01,\n",
              "                        9.6745e-02, -2.4196e-01, -8.3027e-02,  8.6518e-02, -1.6165e-02,\n",
              "                        1.4271e-01, -2.3588e-02,  2.1574e-02, -4.4806e-03, -5.5572e-02],\n",
              "                      [ 3.4954e-02, -1.0149e-02, -4.3702e-03,  8.1820e-02, -1.7186e-01,\n",
              "                        8.5109e-02, -1.0688e-01, -6.4926e-02,  1.8278e-01, -7.2624e-02,\n",
              "                        1.5709e-01, -5.0064e-02,  4.6419e-02,  1.0179e-01, -1.7940e-01,\n",
              "                        4.3252e-02,  8.3324e-02, -8.0227e-02, -1.6732e-02, -6.8887e-02,\n",
              "                       -1.0218e-01, -1.2744e-01, -1.7565e-01, -9.1257e-02,  1.7193e-01,\n",
              "                       -4.5304e-02, -1.9532e-01,  4.6291e-02, -1.2917e-01,  6.4114e-02,\n",
              "                        1.4331e-02, -5.5328e-02, -1.2612e-01,  1.3353e-02, -2.9586e-02,\n",
              "                        1.1905e-02, -1.9858e-02,  8.6127e-02, -4.0006e-02,  1.4688e-01,\n",
              "                       -1.5644e-02, -1.5723e-01,  9.8865e-02, -1.8676e-01, -1.0285e-01,\n",
              "                       -1.9746e-01,  1.3128e-01, -1.7099e-01,  1.0938e-01,  3.8837e-02],\n",
              "                      [ 6.8265e-02, -1.3309e-01, -1.9385e-01,  9.5668e-02,  1.3183e-01,\n",
              "                       -1.4510e-01,  2.2506e-02, -7.2981e-02, -1.0249e-02,  3.5432e-02,\n",
              "                       -6.4638e-02, -1.3156e-01,  9.2650e-02,  1.6169e-01, -6.1485e-02,\n",
              "                       -5.8419e-02, -2.0471e-02,  4.3720e-02,  2.5954e-02, -1.2264e-01,\n",
              "                        7.7407e-02,  1.0047e-02,  8.8647e-02, -8.3038e-03, -1.0502e-01,\n",
              "                       -1.2703e-01,  4.3201e-02,  1.3518e-02,  1.3581e-02, -1.2266e-01,\n",
              "                        4.9367e-02, -8.6529e-02, -1.7765e-01, -6.0078e-02, -1.8510e-01,\n",
              "                        1.1074e-01, -1.2240e-02,  3.8376e-02, -1.2359e-01, -1.0661e-02,\n",
              "                        1.6941e-01,  3.3309e-02,  2.4517e-02, -1.3896e-01,  8.9112e-02,\n",
              "                        5.8673e-02, -2.8090e-02,  9.9798e-02,  6.7875e-02, -1.9111e-03],\n",
              "                      [ 1.4249e-01,  1.4405e-02, -1.1474e-01, -7.4363e-02, -1.9647e-01,\n",
              "                       -4.9974e-02, -2.8209e-01, -4.7018e-02, -9.4870e-02,  1.1033e-02,\n",
              "                       -1.0004e-01,  8.0549e-02, -8.0852e-02, -4.4350e-03,  2.0859e-01,\n",
              "                       -2.1634e-01, -6.4173e-02, -1.7924e-01, -8.2949e-02, -7.5798e-02,\n",
              "                       -1.7040e-02,  4.4448e-02, -1.2699e-01,  1.1003e-01, -3.2198e-02,\n",
              "                        2.0538e-01,  1.7115e-01,  8.6223e-02, -4.9000e-02,  1.1307e-01,\n",
              "                        7.9215e-03, -1.4666e-01, -7.1760e-04,  1.8494e-01, -8.5993e-02,\n",
              "                       -3.5035e-02,  2.8373e-02, -1.4037e-01, -1.1187e-01, -1.2615e-01,\n",
              "                       -2.2312e-02, -6.3137e-03, -5.8165e-02,  1.7903e-01, -2.0889e-03,\n",
              "                        1.1411e-02,  7.7408e-02, -1.4227e-01,  4.5723e-02, -2.0499e-01]],\n",
              "                     device='cuda:0')),\n",
              "             ('out.bias',\n",
              "              tensor([-0.0074,  0.0260,  0.0418, -0.0142, -0.0093, -0.0488, -0.1002,  0.0299,\n",
              "                      -0.0358, -0.0776], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(best_model_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKegncSSa8_R",
        "outputId": "b78b8902-5059-46ea-9548-d6c592e3dc11"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'pytorch_sample.ckpt')"
      ],
      "metadata": {
        "id": "2r00epjBbBW-"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('pytorch_sample.ckpt')"
      ],
      "metadata": {
        "id": "XsNNGAIYbFgk"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sed9SAGxbI0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}